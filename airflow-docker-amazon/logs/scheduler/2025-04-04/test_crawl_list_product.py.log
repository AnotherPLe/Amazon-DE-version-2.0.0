[2025-04-04T19:28:30.597+0000] {processor.py:186} INFO - Started process (PID=1378) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:28:30.598+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:28:30.600+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:30.600+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:28:30.888+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:28:31.200+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.200+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:scrape_amazon_dag
[2025-04-04T19:28:31.214+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.214+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:scrape_amazon_dag
[2025-04-04T19:28:31.225+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.225+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:scrape_amazon_dag
[2025-04-04T19:28:31.241+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.240+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:scrape_amazon_dag
[2025-04-04T19:28:31.251+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.251+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:scrape_amazon_dag
[2025-04-04T19:28:31.261+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.261+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:scrape_amazon_dag
[2025-04-04T19:28:31.274+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.274+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:scrape_amazon_dag
[2025-04-04T19:28:31.275+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.275+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:28:31.288+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.288+0000] {dag.py:3262} INFO - Creating ORM DAG for scrape_amazon_dag
[2025-04-04T19:28:31.290+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:28:31.289+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:28:31.321+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.732 seconds
[2025-04-04T19:29:01.390+0000] {processor.py:186} INFO - Started process (PID=1443) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:01.392+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:29:01.394+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:01.393+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:01.636+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:01.842+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:01.842+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:29:01.857+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:01.857+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:29:01.886+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.506 seconds
[2025-04-04T19:29:32.002+0000] {processor.py:186} INFO - Started process (PID=1507) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:32.003+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:29:32.004+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:32.004+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:32.444+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:29:32.465+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:32.465+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:29:32.480+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:29:32.479+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:29:32.508+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.514 seconds
[2025-04-04T19:30:03.234+0000] {processor.py:186} INFO - Started process (PID=1573) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:03.235+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:30:03.237+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:03.237+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:03.752+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:03.769+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:03.769+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:30:03.785+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:03.785+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:30:03.817+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.591 seconds
[2025-04-04T19:30:33.887+0000] {processor.py:186} INFO - Started process (PID=1636) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:33.888+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:30:33.890+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:33.889+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:34.332+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:30:34.351+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:34.351+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:30:34.366+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:30:34.365+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:30:34.390+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.511 seconds
[2025-04-04T19:31:05.036+0000] {processor.py:186} INFO - Started process (PID=1702) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:05.037+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:31:05.039+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:05.038+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:05.446+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:05.467+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:05.467+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:31:05.484+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:05.484+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:31:05.519+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.491 seconds
[2025-04-04T19:31:32.993+0000] {processor.py:186} INFO - Started process (PID=1765) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:32.995+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:31:32.996+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:32.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:33.487+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:33.466+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 266, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:31:33.490+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:33.550+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.564 seconds
[2025-04-04T19:31:34.177+0000] {processor.py:186} INFO - Started process (PID=1766) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:34.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:31:34.181+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:34.180+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:34.718+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:31:34.711+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 266, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:31:34.721+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:31:34.760+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.591 seconds
[2025-04-04T19:32:05.776+0000] {processor.py:186} INFO - Started process (PID=1830) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:05.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:32:05.780+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:32:05.779+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:06.187+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:32:06.178+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 266, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:32:06.191+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:06.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.447 seconds
[2025-04-04T19:32:29.349+0000] {processor.py:186} INFO - Started process (PID=1889) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:29.351+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:32:29.353+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:32:29.352+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:29.842+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:32:29.835+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 267, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:32:29.846+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:32:29.908+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.569 seconds
[2025-04-04T19:33:00.320+0000] {processor.py:186} INFO - Started process (PID=1952) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:00.322+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:33:00.324+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:00.324+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:00.779+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:00.774+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 267, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:33:00.783+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:00.806+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.496 seconds
[2025-04-04T19:33:31.463+0000] {processor.py:186} INFO - Started process (PID=2016) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:31.464+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:33:31.466+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:31.465+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:31.907+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:31.901+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 267, in <module>
    'retry_delay': datetime.timedelta(minutes=10),
                   ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'datetime.datetime' has no attribute 'timedelta'
[2025-04-04T19:33:31.910+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:31.936+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.483 seconds
[2025-04-04T19:33:58.050+0000] {processor.py:186} INFO - Started process (PID=2070) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:58.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:33:58.052+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:58.052+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:58.561+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:33:58.717+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:58.716+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:33:58.739+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:33:58.739+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:33:58.826+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.785 seconds
[2025-04-04T19:34:29.169+0000] {processor.py:186} INFO - Started process (PID=2135) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:34:29.170+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:34:29.172+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:34:29.171+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:34:29.412+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:34:29.438+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:34:29.437+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:34:29.456+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:34:29.456+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:34:29.483+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.322 seconds
[2025-04-04T19:35:00.068+0000] {processor.py:186} INFO - Started process (PID=2199) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:00.069+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:35:00.072+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:00.071+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:00.343+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:00.372+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:00.371+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:35:00.393+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:00.392+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:35:00.422+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.364 seconds
[2025-04-04T19:35:31.256+0000] {processor.py:186} INFO - Started process (PID=2264) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:31.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:35:31.259+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:31.258+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:31.494+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:35:31.516+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:31.515+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:35:31.535+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:35:31.535+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:35:31.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.318 seconds
[2025-04-04T19:36:01.651+0000] {processor.py:186} INFO - Started process (PID=2330) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:01.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:36:01.654+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:01.654+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:01.911+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:01.931+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:01.931+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:36:01.949+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:01.949+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:36:01.980+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.336 seconds
[2025-04-04T19:36:32.181+0000] {processor.py:186} INFO - Started process (PID=2393) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:32.182+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:36:32.184+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:32.184+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:32.448+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:36:32.490+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:32.489+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:36:32.521+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:36:32.521+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:36:32.562+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.389 seconds
[2025-04-04T19:37:02.785+0000] {processor.py:186} INFO - Started process (PID=2457) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:02.787+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:37:02.789+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:02.788+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:03.056+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:03.080+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:03.079+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:37:03.106+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:03.105+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:37:03.136+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.361 seconds
[2025-04-04T19:37:33.206+0000] {processor.py:186} INFO - Started process (PID=2523) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:33.208+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:37:33.209+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:33.209+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:33.487+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:37:33.508+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:33.507+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:37:33.533+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:37:33.533+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:37:33.567+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.368 seconds
[2025-04-04T19:38:03.765+0000] {processor.py:186} INFO - Started process (PID=2585) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:03.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:38:03.768+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:03.767+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:04.031+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:04.056+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:04.055+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:38:04.073+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:04.073+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:38:04.107+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.351 seconds
[2025-04-04T19:38:34.563+0000] {processor.py:186} INFO - Started process (PID=2649) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:34.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:38:34.566+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:34.566+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:34.825+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:38:34.850+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:34.849+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:38:34.867+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:38:34.867+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:38:34.899+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.344 seconds
[2025-04-04T19:39:05.141+0000] {processor.py:186} INFO - Started process (PID=2713) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:05.142+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:39:05.145+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:05.144+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:05.396+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:05.423+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:05.423+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:39:05.440+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:05.439+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:39:05.470+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.338 seconds
[2025-04-04T19:39:35.560+0000] {processor.py:186} INFO - Started process (PID=2779) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:35.561+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:39:35.563+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:35.563+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:35.796+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:35.819+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:35.818+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:39:35.837+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:35.837+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:39:35.866+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.314 seconds
[2025-04-04T19:39:58.481+0000] {processor.py:186} INFO - Started process (PID=2808) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:58.482+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:39:58.484+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:58.484+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:58.734+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:39:58.758+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:58.758+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:39:58.786+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:39:58.786+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:39:58.831+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.358 seconds
[2025-04-04T19:40:29.889+0000] {processor.py:186} INFO - Started process (PID=2872) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:40:29.890+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:40:29.892+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:40:29.892+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:40:30.159+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:40:30.186+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:40:30.185+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:40:30.215+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:40:30.215+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:40:30.246+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.365 seconds
[2025-04-04T19:41:00.424+0000] {processor.py:186} INFO - Started process (PID=2935) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:00.426+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:41:00.427+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:00.427+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:00.665+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:00.686+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:00.686+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:41:00.703+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:00.703+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:41:00.734+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.322 seconds
[2025-04-04T19:41:31.068+0000] {processor.py:186} INFO - Started process (PID=2999) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:31.069+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:41:31.072+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:31.071+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:31.342+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:41:31.366+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:31.366+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:41:31.385+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:41:31.385+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:41:31.417+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.360 seconds
[2025-04-04T19:42:01.489+0000] {processor.py:186} INFO - Started process (PID=3063) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:01.490+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:42:01.492+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:01.492+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:01.783+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:01.806+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:01.805+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:42:01.825+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:01.824+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:42:01.860+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.379 seconds
[2025-04-04T19:42:32.244+0000] {processor.py:186} INFO - Started process (PID=3127) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:32.246+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:42:32.248+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:32.247+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:32.533+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:42:32.552+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:32.552+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:42:32.572+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:42:32.572+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:42:32.600+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.363 seconds
[2025-04-04T19:43:03.025+0000] {processor.py:186} INFO - Started process (PID=3191) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:03.026+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:43:03.028+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:03.028+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:03.286+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:03.310+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:03.309+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:43:03.328+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:03.328+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:43:03.355+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.338 seconds
[2025-04-04T19:43:33.438+0000] {processor.py:186} INFO - Started process (PID=3254) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:33.440+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:43:33.441+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:33.441+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:33.722+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:43:33.745+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:33.745+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:43:33.764+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:43:33.763+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:43:33.799+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.370 seconds
[2025-04-04T19:44:03.968+0000] {processor.py:186} INFO - Started process (PID=3318) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:03.970+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:44:03.973+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:03.972+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:04.227+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:04.249+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:04.249+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:44:04.266+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:04.266+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:44:04.298+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.338 seconds
[2025-04-04T19:44:35.064+0000] {processor.py:186} INFO - Started process (PID=3384) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:35.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:44:35.067+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:35.066+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:35.320+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:44:35.343+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:35.342+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:44:35.358+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:44:35.358+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:44:35.386+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.331 seconds
[2025-04-04T19:45:06.103+0000] {processor.py:186} INFO - Started process (PID=3448) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:06.105+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:45:06.108+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:06.107+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:06.420+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:06.442+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:06.442+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:45:06.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:06.458+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:45:06.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.394 seconds
[2025-04-04T19:45:37.065+0000] {processor.py:186} INFO - Started process (PID=3513) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:37.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:45:37.068+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:37.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:37.333+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:45:37.359+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:37.358+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:45:37.375+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:45:37.375+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:45:37.403+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.347 seconds
[2025-04-04T19:46:07.547+0000] {processor.py:186} INFO - Started process (PID=3577) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:07.548+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:46:07.550+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:07.549+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:07.884+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:07.908+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:07.907+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:46:07.936+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:07.936+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:46:07.964+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.427 seconds
[2025-04-04T19:46:38.189+0000] {processor.py:186} INFO - Started process (PID=3643) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:38.190+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:46:38.193+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:38.192+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:38.534+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:46:38.558+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:38.557+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:46:38.579+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:46:38.579+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:46:38.612+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.432 seconds
[2025-04-04T19:47:08.906+0000] {processor.py:186} INFO - Started process (PID=3706) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:08.907+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:47:08.909+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:08.909+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:09.190+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:09.220+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:09.219+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:47:09.248+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:09.247+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:47:09.310+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.415 seconds
[2025-04-04T19:47:39.452+0000] {processor.py:186} INFO - Started process (PID=3770) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:39.453+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:47:39.455+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:39.455+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:39.693+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:47:39.717+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:39.716+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:47:39.734+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:47:39.733+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:47:39.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.323 seconds
[2025-04-04T19:48:10.459+0000] {processor.py:186} INFO - Started process (PID=3834) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:10.461+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:48:10.463+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:10.463+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:10.732+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:10.756+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:10.755+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:48:10.777+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:10.776+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:48:10.819+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.375 seconds
[2025-04-04T19:48:26.080+0000] {processor.py:186} INFO - Started process (PID=3896) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:26.081+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:48:26.083+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:26.082+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:26.395+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:26.388+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\csv\\src_input\\amazon_categories_full copy.csv'
[2025-04-04T19:48:26.397+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:26.429+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.357 seconds
[2025-04-04T19:48:57.491+0000] {processor.py:186} INFO - Started process (PID=3960) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:57.493+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:48:57.496+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:57.495+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:57.748+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:48:57.741+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\csv\\src_input\\amazon_categories_full copy.csv'
[2025-04-04T19:48:57.750+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:48:57.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.308 seconds
[2025-04-04T19:49:25.325+0000] {processor.py:186} INFO - Started process (PID=4015) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:25.329+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:49:25.335+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:49:25.335+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:25.634+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:49:25.628+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\\\csv\\\\src_input\\\\amazon_categories_full copy.csv'
[2025-04-04T19:49:25.636+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:25.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.365 seconds
[2025-04-04T19:49:55.856+0000] {processor.py:186} INFO - Started process (PID=4079) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:55.857+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:49:55.859+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:49:55.858+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:56.109+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:49:56.103+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\\\csv\\\\src_input\\\\amazon_categories_full copy.csv'
[2025-04-04T19:49:56.111+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:49:56.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.290 seconds
[2025-04-04T19:50:26.290+0000] {processor.py:186} INFO - Started process (PID=4143) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:26.291+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:50:26.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:50:26.293+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:26.602+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:50:26.596+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\\\csv\\\\src_input\\\\amazon_categories_full copy.csv'
[2025-04-04T19:50:26.603+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:26.633+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.353 seconds
[2025-04-04T19:50:56.713+0000] {processor.py:186} INFO - Started process (PID=4206) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:56.714+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:50:56.716+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:50:56.715+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:56.925+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:50:56.919+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'airflow-docker-amazon\\\\csv\\\\src_input\\\\amazon_categories_full copy.csv'
[2025-04-04T19:50:56.927+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:50:56.954+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.250 seconds
[2025-04-04T19:51:03.165+0000] {processor.py:186} INFO - Started process (PID=4216) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:03.166+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:51:03.168+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:03.167+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:03.415+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:03.407+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/dags/airflow-docker-amazon/csv/src_input/amazon_categories_full copy.csv'
[2025-04-04T19:51:03.417+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:03.447+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.292 seconds
[2025-04-04T19:51:33.972+0000] {processor.py:186} INFO - Started process (PID=4274) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:33.973+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:51:33.975+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:33.974+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:34.253+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:34.247+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 293, in <module>
    urls = read_urls_from_csv(csv_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/test_crawl_list_product.py", line 273, in read_urls_from_csv
    with open(csv_file, newline='', encoding='utf-8') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/dags/airflow-docker-amazon/csv/src_input/amazon_categories_full copy.csv'
[2025-04-04T19:51:34.256+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:34.288+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.324 seconds
[2025-04-04T19:51:56.772+0000] {processor.py:186} INFO - Started process (PID=4332) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:56.773+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:51:56.777+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:56.776+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:57.104+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:51:57.094+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:51:57.111+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:51:57.136+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.381 seconds
[2025-04-04T19:52:27.988+0000] {processor.py:186} INFO - Started process (PID=4396) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:27.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:52:27.992+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:52:27.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:28.281+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:52:28.274+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:52:28.283+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:28.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T19:52:58.541+0000] {processor.py:186} INFO - Started process (PID=4461) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:58.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:52:58.543+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:52:58.543+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:58.793+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:52:58.785+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:52:58.794+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:52:58.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.285 seconds
[2025-04-04T19:53:29.212+0000] {processor.py:186} INFO - Started process (PID=4525) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:53:29.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:53:29.216+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:53:29.215+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:53:29.547+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:53:29.541+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:53:29.549+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:53:29.582+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.377 seconds
[2025-04-04T19:54:00.460+0000] {processor.py:186} INFO - Started process (PID=4589) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:00.461+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:54:00.464+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:54:00.463+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:00.710+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:54:00.702+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:54:00.711+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:00.743+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.292 seconds
[2025-04-04T19:54:31.021+0000] {processor.py:186} INFO - Started process (PID=4652) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:31.022+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:54:31.024+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:54:31.023+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:31.267+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:54:31.260+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:54:31.268+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:54:31.298+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.286 seconds
[2025-04-04T19:55:02.046+0000] {processor.py:186} INFO - Started process (PID=4716) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:02.048+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:55:02.049+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:55:02.049+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:02.303+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:55:02.296+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:55:02.304+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:02.340+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.305 seconds
[2025-04-04T19:55:32.728+0000] {processor.py:186} INFO - Started process (PID=4780) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:32.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:55:32.731+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:55:32.731+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:33.066+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:55:33.060+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:55:33.067+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:55:33.101+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.383 seconds
[2025-04-04T19:56:03.332+0000] {processor.py:186} INFO - Started process (PID=4844) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:03.334+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:56:03.337+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:56:03.336+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:03.588+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:56:03.580+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 297, in <module>
    scrape_task = PythonOperator(
                  ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 222, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'scrape_data_ URL' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2025-04-04T19:56:03.590+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:03.617+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.298 seconds
[2025-04-04T19:56:33.606+0000] {processor.py:186} INFO - Started process (PID=4908) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:33.607+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:56:33.609+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:56:33.608+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:33.856+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:56:33.998+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:56:33.997+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:56:34.014+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:56:34.014+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:56:34.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.464 seconds
[2025-04-04T19:57:04.430+0000] {processor.py:186} INFO - Started process (PID=4972) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:04.431+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:57:04.433+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:04.432+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:04.675+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:04.696+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:04.696+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:57:04.713+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:04.713+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:57:04.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.322 seconds
[2025-04-04T19:57:34.817+0000] {processor.py:186} INFO - Started process (PID=5036) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:34.818+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:57:34.819+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:34.819+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:35.043+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:57:35.066+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:35.066+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:57:35.085+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:57:35.085+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:57:35.121+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.312 seconds
[2025-04-04T19:58:06.044+0000] {processor.py:186} INFO - Started process (PID=5101) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:06.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:58:06.049+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:06.048+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:06.355+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:06.378+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:06.377+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:58:06.395+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:06.395+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:58:06.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.395 seconds
[2025-04-04T19:58:36.642+0000] {processor.py:186} INFO - Started process (PID=5165) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:36.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:58:36.646+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:36.645+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:37.001+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:58:37.022+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:37.022+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:58:37.038+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:58:37.038+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:58:37.066+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.432 seconds
[2025-04-04T19:59:07.255+0000] {processor.py:186} INFO - Started process (PID=5229) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:07.256+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:59:07.259+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:07.258+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:07.557+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:07.586+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:07.585+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:59:07.609+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:07.609+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:59:07.642+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.395 seconds
[2025-04-04T19:59:10.746+0000] {processor.py:186} INFO - Started process (PID=5230) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:10.747+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:59:10.749+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:10.749+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:11.018+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:11.154+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:11.153+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:59:11.174+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:11.173+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:59:11.217+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.480 seconds
[2025-04-04T19:59:42.105+0000] {processor.py:186} INFO - Started process (PID=5294) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:42.106+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T19:59:42.108+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:42.108+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:42.345+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T19:59:42.365+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:42.365+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T19:59:42.382+0000] {logging_mixin.py:190} INFO - [2025-04-04T19:59:42.382+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T19:59:42.411+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.316 seconds
[2025-04-04T20:00:05.265+0000] {processor.py:186} INFO - Started process (PID=5358) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:05.266+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:00:05.268+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:05.268+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:05.561+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:05.583+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:05.582+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:00:05.604+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:05.603+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:00:05.641+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.384 seconds
[2025-04-04T20:00:35.766+0000] {processor.py:186} INFO - Started process (PID=5421) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:35.767+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:00:35.769+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:35.768+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:36.000+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:00:36.024+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:36.024+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:00:36.045+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:00:36.044+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:00:36.077+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.318 seconds
[2025-04-04T20:01:06.824+0000] {processor.py:186} INFO - Started process (PID=5485) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:06.825+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:01:06.827+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:06.826+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:07.139+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:07.160+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:07.160+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:01:07.180+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:07.180+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:01:07.213+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.397 seconds
[2025-04-04T20:01:37.363+0000] {processor.py:186} INFO - Started process (PID=5549) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:37.364+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:01:37.365+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:37.365+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:37.613+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:01:37.637+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:37.636+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:01:37.654+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:01:37.654+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:01:37.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.330 seconds
[2025-04-04T20:02:08.548+0000] {processor.py:186} INFO - Started process (PID=5613) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:08.550+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:02:08.551+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:08.551+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:08.795+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:08.816+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:08.816+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:02:08.835+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:08.834+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:02:08.866+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.328 seconds
[2025-04-04T20:02:39.107+0000] {processor.py:186} INFO - Started process (PID=5677) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:39.109+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:02:39.112+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:39.111+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:39.372+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:02:39.397+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:39.397+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:02:39.415+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:02:39.415+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:02:39.451+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T20:03:04.239+0000] {processor.py:186} INFO - Started process (PID=5728) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:04.241+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:03:04.242+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:04.242+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:04.503+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:04.637+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:04.637+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:03:04.652+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:04.651+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:03:04.690+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.458 seconds
[2025-04-04T20:03:34.956+0000] {processor.py:186} INFO - Started process (PID=5792) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:34.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:03:34.959+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:34.958+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:35.213+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:03:35.236+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:35.236+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:03:35.255+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:03:35.254+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:03:35.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.335 seconds
[2025-04-04T20:04:01.707+0000] {processor.py:186} INFO - Started process (PID=5830) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:01.709+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:04:01.712+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:01.711+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:02.051+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:02.698+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:02.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:04:02.746+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:02.746+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:04:02.883+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.187 seconds
[2025-04-04T20:04:33.888+0000] {processor.py:186} INFO - Started process (PID=5895) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:33.897+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:04:33.903+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:33.903+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:34.205+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:04:34.423+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:34.423+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:04:34.439+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:04:34.439+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:04:34.466+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.599 seconds
[2025-04-04T20:05:04.602+0000] {processor.py:186} INFO - Started process (PID=5961) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:04.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:05:04.605+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:04.604+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:04.859+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:05.090+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:05.090+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:05:05.110+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:05.109+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:05:05.142+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.548 seconds
[2025-04-04T20:05:35.278+0000] {processor.py:186} INFO - Started process (PID=6021) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:35.280+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:05:35.283+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:35.282+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:35.545+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:05:35.571+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:35.571+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:05:35.593+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:05:35.593+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:05:35.626+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.356 seconds
[2025-04-04T20:06:06.187+0000] {processor.py:186} INFO - Started process (PID=6085) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:06.188+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:06:06.190+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:06.189+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:06.423+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:06.445+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:06.444+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:06:06.466+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:06.466+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:06:06.498+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.321 seconds
[2025-04-04T20:06:36.852+0000] {processor.py:186} INFO - Started process (PID=6149) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:36.853+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:06:36.856+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:36.855+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:37.111+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:06:37.137+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:37.136+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:06:37.163+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:06:37.163+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:06:37.404+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.563 seconds
[2025-04-04T20:07:07.562+0000] {processor.py:186} INFO - Started process (PID=6216) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:07.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:07:07.565+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:07.564+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:07.807+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:07.831+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:07.830+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:07:07.863+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:07.862+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:07:07.929+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.375 seconds
[2025-04-04T20:07:38.121+0000] {processor.py:186} INFO - Started process (PID=6276) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:38.122+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:07:38.124+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:38.124+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:38.399+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:07:38.419+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:38.418+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:07:38.437+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:07:38.436+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:07:38.468+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T20:08:08.664+0000] {processor.py:186} INFO - Started process (PID=6342) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:08.665+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:08:08.667+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:08.667+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:09.106+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:09.127+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:09.126+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:08:09.145+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:09.144+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:08:09.173+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.522 seconds
[2025-04-04T20:08:40.063+0000] {processor.py:186} INFO - Started process (PID=6409) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:40.064+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:08:40.066+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:40.066+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:40.332+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:08:40.355+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:40.355+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:08:40.376+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:08:40.376+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:08:40.407+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T20:09:10.471+0000] {processor.py:186} INFO - Started process (PID=6473) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:10.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:09:10.474+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:10.473+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:10.926+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:10.944+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:10.944+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:09:10.960+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:10.960+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:09:10.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.527 seconds
[2025-04-04T20:09:41.550+0000] {processor.py:186} INFO - Started process (PID=6536) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:41.552+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:09:41.553+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:41.553+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:41.960+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:09:41.979+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:41.979+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:09:41.993+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:09:41.993+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:09:42.022+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.479 seconds
[2025-04-04T20:10:12.134+0000] {processor.py:186} INFO - Started process (PID=6599) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:12.136+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:10:12.137+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:12.137+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:12.594+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:12.612+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:12.611+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:10:12.628+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:12.627+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:10:12.655+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.528 seconds
[2025-04-04T20:10:43.544+0000] {processor.py:186} INFO - Started process (PID=6663) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:43.546+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:10:43.549+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:43.548+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:44.003+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:10:44.027+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:44.026+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:10:44.041+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:10:44.041+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:10:44.076+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.541 seconds
[2025-04-04T20:11:14.193+0000] {processor.py:186} INFO - Started process (PID=6727) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:14.195+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:11:14.196+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:14.196+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:14.652+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:14.671+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:14.671+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:11:14.686+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:14.686+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:11:14.715+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.533 seconds
[2025-04-04T20:11:44.845+0000] {processor.py:186} INFO - Started process (PID=6791) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:44.846+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:11:44.848+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:44.847+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:45.264+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:11:45.285+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:45.284+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:11:45.305+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:11:45.304+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:11:45.341+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.507 seconds
[2025-04-04T20:12:15.555+0000] {processor.py:186} INFO - Started process (PID=6855) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:15.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:12:15.558+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:15.558+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:15.996+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:16.021+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:16.020+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:12:16.036+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:16.036+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:12:16.063+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.518 seconds
[2025-04-04T20:12:46.491+0000] {processor.py:186} INFO - Started process (PID=6921) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:46.493+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:12:46.495+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:46.494+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:47.314+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:12:47.332+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:47.332+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:12:47.347+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:12:47.347+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:12:47.385+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.904 seconds
[2025-04-04T20:13:17.839+0000] {processor.py:186} INFO - Started process (PID=6993) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:17.840+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:13:17.842+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:17.842+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:18.274+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:18.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:18.293+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:13:18.307+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:18.307+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:13:18.335+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.503 seconds
[2025-04-04T20:13:49.319+0000] {processor.py:186} INFO - Started process (PID=7058) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:49.321+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:13:49.323+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:49.322+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:49.799+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:13:49.822+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:49.822+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:13:49.836+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:13:49.836+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:13:49.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.557 seconds
[2025-04-04T20:14:19.979+0000] {processor.py:186} INFO - Started process (PID=7123) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:19.980+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:14:19.982+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:19.982+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:20.206+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:20.236+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:20.235+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:14:20.253+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:20.253+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:14:20.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.314 seconds
[2025-04-04T20:14:50.360+0000] {processor.py:186} INFO - Started process (PID=7188) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:50.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:14:50.364+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:50.363+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:50.683+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:14:50.710+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:50.709+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:14:50.735+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:14:50.734+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:14:50.769+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.417 seconds
[2025-04-04T20:15:21.522+0000] {processor.py:186} INFO - Started process (PID=7253) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:21.523+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:15:21.525+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:21.525+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:21.838+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:21.876+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:21.875+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:15:21.906+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:21.906+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:15:21.942+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.433 seconds
[2025-04-04T20:15:52.088+0000] {processor.py:186} INFO - Started process (PID=7321) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:52.089+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:15:52.092+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:52.091+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:52.355+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:15:52.377+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:52.377+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:15:52.396+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:15:52.396+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:15:52.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.348 seconds
[2025-04-04T20:16:22.751+0000] {processor.py:186} INFO - Started process (PID=7385) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:22.753+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:16:22.754+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:22.754+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:23.052+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:23.074+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:23.073+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:16:23.093+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:23.093+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:16:23.126+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.384 seconds
[2025-04-04T20:16:53.298+0000] {processor.py:186} INFO - Started process (PID=7450) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:53.300+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:16:53.302+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:53.302+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:53.563+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:16:53.591+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:53.590+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:16:53.611+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:16:53.611+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:16:53.644+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.358 seconds
[2025-04-04T20:17:23.713+0000] {processor.py:186} INFO - Started process (PID=7515) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:23.714+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:17:23.716+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:23.715+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:23.951+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:23.974+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:23.974+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:17:23.990+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:23.990+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:17:24.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.315 seconds
[2025-04-04T20:17:54.115+0000] {processor.py:186} INFO - Started process (PID=7579) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:54.116+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:17:54.118+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:54.117+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:54.377+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:17:54.399+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:54.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:17:54.419+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:17:54.419+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:17:54.456+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.352 seconds
[2025-04-04T20:18:24.744+0000] {processor.py:186} INFO - Started process (PID=7644) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:24.745+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:18:24.748+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:24.747+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:25.055+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:25.101+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:25.099+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:18:25.130+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:25.129+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:18:25.158+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.427 seconds
[2025-04-04T20:18:55.405+0000] {processor.py:186} INFO - Started process (PID=7709) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:55.406+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:18:55.408+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:55.408+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:55.668+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:18:55.700+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:55.699+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:18:55.723+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:18:55.722+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:18:55.760+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.365 seconds
[2025-04-04T20:19:25.950+0000] {processor.py:186} INFO - Started process (PID=7776) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:25.951+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:19:25.953+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:25.953+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:26.211+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:26.232+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:26.232+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:19:26.251+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:26.251+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:19:26.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.340 seconds
[2025-04-04T20:19:56.453+0000] {processor.py:186} INFO - Started process (PID=7842) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:56.454+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:19:56.456+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:56.456+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:56.697+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:19:56.723+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:56.723+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:19:56.740+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:19:56.740+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:19:56.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.327 seconds
[2025-04-04T20:20:26.927+0000] {processor.py:186} INFO - Started process (PID=7908) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:26.928+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:20:26.930+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:26.929+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:27.194+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:27.216+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:27.215+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:20:27.234+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:27.234+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:20:27.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.356 seconds
[2025-04-04T20:20:57.652+0000] {processor.py:186} INFO - Started process (PID=7973) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:57.654+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:20:57.656+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:57.656+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:58.008+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:20:58.035+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:58.034+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:20:58.057+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:20:58.057+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:20:58.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.454 seconds
[2025-04-04T20:21:28.507+0000] {processor.py:186} INFO - Started process (PID=8036) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:28.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:21:28.510+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:28.510+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:28.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:28.750+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:28.749+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:21:28.765+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:28.765+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:21:28.788+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.288 seconds
[2025-04-04T20:21:58.967+0000] {processor.py:186} INFO - Started process (PID=8100) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:58.968+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:21:58.969+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:58.969+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:59.211+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:21:59.232+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:59.232+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:21:59.249+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:21:59.248+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:21:59.279+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.320 seconds
[2025-04-04T20:22:29.486+0000] {processor.py:186} INFO - Started process (PID=8166) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:22:29.487+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:22:29.489+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:22:29.488+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:22:29.756+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:22:29.781+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:22:29.780+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:22:29.799+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:22:29.799+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:22:29.836+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.358 seconds
[2025-04-04T20:22:59.961+0000] {processor.py:186} INFO - Started process (PID=8229) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:22:59.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:22:59.964+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:22:59.964+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:23:00.266+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:23:00.292+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:23:00.291+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:23:00.312+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:23:00.312+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:23:00.345+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.394 seconds
[2025-04-04T20:23:30.462+0000] {processor.py:186} INFO - Started process (PID=8295) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:23:30.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:23:30.464+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:23:30.464+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:23:30.730+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:23:30.757+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:23:30.755+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:23:30.779+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:23:30.779+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:23:30.810+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.356 seconds
[2025-04-04T20:24:00.167+0000] {processor.py:186} INFO - Started process (PID=8356) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:00.169+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:24:00.173+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:00.172+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:00.621+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:00.620+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:24:00.626+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:00.802+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:00.801+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:24:00.828+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:00.827+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:24:00.863+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.708 seconds
[2025-04-04T20:24:30.928+0000] {processor.py:186} INFO - Started process (PID=8423) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:30.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:24:30.931+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:30.930+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:31.192+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:31.192+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:24:31.195+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:24:31.219+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:31.218+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:24:31.242+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:24:31.241+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:24:31.279+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.360 seconds
[2025-04-04T20:25:01.471+0000] {processor.py:186} INFO - Started process (PID=8489) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:01.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:25:01.474+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:01.473+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:01.746+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:01.746+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:25:01.750+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:01.776+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:01.775+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:25:01.797+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:01.797+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:25:01.826+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.362 seconds
[2025-04-04T20:25:27.715+0000] {processor.py:186} INFO - Started process (PID=8537) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:27.716+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:25:27.719+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:27.718+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:28.057+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:28.057+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:25:28.061+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:28.096+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:28.095+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:25:28.124+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:28.124+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:25:28.215+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.511 seconds
[2025-04-04T20:25:47.959+0000] {processor.py:186} INFO - Started process (PID=8581) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:47.961+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:25:47.963+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:47.962+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:48.287+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:48.286+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:25:48.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:25:48.288+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:25:48.295+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:25:48.324+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.378 seconds
[2025-04-04T20:26:03.204+0000] {processor.py:186} INFO - Started process (PID=8620) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:03.205+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:03.207+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:03.207+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:03.503+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:03.503+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:03.523+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:03.504+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:03.526+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:03.553+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.356 seconds
[2025-04-04T20:26:04.360+0000] {processor.py:186} INFO - Started process (PID=8621) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:04.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:04.363+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:04.363+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:04.661+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:04.661+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:04.667+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:04.662+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:04.670+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:04.694+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.342 seconds
[2025-04-04T20:26:19.637+0000] {processor.py:186} INFO - Started process (PID=8650) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:19.638+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:19.640+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:19.639+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:19.916+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:19.916+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:19.943+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:19.917+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:19.947+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:19.975+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.347 seconds
[2025-04-04T20:26:20.695+0000] {processor.py:186} INFO - Started process (PID=8657) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:20.696+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:20.698+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:20.698+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:20.950+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:20.950+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:20.957+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:20.951+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:20.960+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:20.988+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.300 seconds
[2025-04-04T20:26:51.985+0000] {processor.py:186} INFO - Started process (PID=8722) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:51.993+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:51.996+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:51.995+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:52.341+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:52.340+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:52.346+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:52.341+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:52.349+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:52.378+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.407 seconds
[2025-04-04T20:26:57.986+0000] {processor.py:186} INFO - Started process (PID=8732) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:57.987+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:26:57.990+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:57.989+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:58.406+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:58.406+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:26:58.413+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:26:58.407+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:26:58.416+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:26:58.446+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.471 seconds
[2025-04-04T20:27:28.573+0000] {processor.py:186} INFO - Started process (PID=8797) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:28.574+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:27:28.576+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:28.575+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:28.865+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:28.865+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:27:28.872+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:28.866+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:27:28.875+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:28.906+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.344 seconds
[2025-04-04T20:27:59.013+0000] {processor.py:186} INFO - Started process (PID=8863) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:59.014+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:27:59.016+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:59.016+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:59.278+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:59.278+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:27:59.284+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:27:59.279+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 299, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:27:59.286+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:27:59.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.313 seconds
[2025-04-04T20:28:06.206+0000] {processor.py:186} INFO - Started process (PID=8884) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:06.207+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:28:06.209+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:06.208+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:06.494+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:06.494+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:28:06.498+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:06.718+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:06.717+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:28:06.743+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:06.743+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:28:06.799+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.600 seconds
[2025-04-04T20:28:37.688+0000] {processor.py:186} INFO - Started process (PID=8949) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:37.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:28:37.692+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:37.692+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:38.023+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:38.023+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:28:38.028+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:28:38.055+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:38.055+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:28:38.078+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:28:38.077+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:28:38.110+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.430 seconds
[2025-04-04T20:29:08.387+0000] {processor.py:186} INFO - Started process (PID=9014) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:08.388+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:29:08.390+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:08.390+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:08.661+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:08.660+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:29:08.663+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:08.680+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:08.679+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:29:08.700+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:08.699+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:29:08.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.350 seconds
[2025-04-04T20:29:37.694+0000] {processor.py:186} INFO - Started process (PID=9079) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:37.695+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:29:37.697+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:37.696+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:37.947+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:37.946+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:29:37.949+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:29:37.971+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:37.971+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:29:37.998+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:29:37.998+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:29:38.037+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.350 seconds
[2025-04-04T20:30:02.250+0000] {processor.py:186} INFO - Started process (PID=9125) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:02.253+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:30:02.256+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:02.255+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:02.718+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:02.717+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:30:02.722+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:02.754+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:02.753+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:30:02.784+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:02.783+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:30:02.825+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.584 seconds
[2025-04-04T20:30:33.361+0000] {processor.py:186} INFO - Started process (PID=9190) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:33.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:30:33.364+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:33.364+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:33.634+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:33.634+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:30:33.637+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:30:33.663+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:33.662+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:30:33.684+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:30:33.684+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:30:33.713+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.361 seconds
[2025-04-04T20:31:03.775+0000] {processor.py:186} INFO - Started process (PID=9256) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:03.776+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:31:03.778+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:03.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:04.018+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:04.017+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:31:04.020+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:04.041+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:04.040+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:31:04.060+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:04.059+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:31:04.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.327 seconds
[2025-04-04T20:31:34.271+0000] {processor.py:186} INFO - Started process (PID=9322) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:34.272+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:31:34.274+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:34.273+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:34.548+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:34.547+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:31:34.550+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:34.571+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:34.570+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:31:34.592+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:34.592+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:31:34.624+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.361 seconds
[2025-04-04T20:31:37.378+0000] {processor.py:186} INFO - Started process (PID=9324) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:37.380+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:31:37.433+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:37.432+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:37.891+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:37.891+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:31:37.894+0000] {processor.py:925} INFO - DAG(s) 'process_list_product_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:38.034+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.033+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:process_list_product_dag
[2025-04-04T20:31:38.052+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.051+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:process_list_product_dag
[2025-04-04T20:31:38.065+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.065+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:process_list_product_dag
[2025-04-04T20:31:38.081+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.081+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:process_list_product_dag
[2025-04-04T20:31:38.094+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.093+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:process_list_product_dag
[2025-04-04T20:31:38.111+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.110+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:process_list_product_dag
[2025-04-04T20:31:38.130+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.129+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:process_list_product_dag
[2025-04-04T20:31:38.131+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.131+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:31:38.161+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.160+0000] {dag.py:3262} INFO - Creating ORM DAG for process_list_product_dag
[2025-04-04T20:31:38.162+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:38.161+0000] {dag.py:4180} INFO - Setting next_dagrun for process_list_product_dag to None, run_after=None
[2025-04-04T20:31:38.198+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.834 seconds
[2025-04-04T20:31:45.933+0000] {processor.py:186} INFO - Started process (PID=9352) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:45.934+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:31:45.936+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:45.935+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:46.196+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:46.196+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:31:46.198+0000] {processor.py:925} INFO - DAG(s) 'process_list_product_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:31:46.215+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:46.214+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:31:46.231+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:31:46.231+0000] {dag.py:4180} INFO - Setting next_dagrun for process_list_product_dag to None, run_after=None
[2025-04-04T20:31:46.268+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.342 seconds
[2025-04-04T20:32:15.520+0000] {processor.py:186} INFO - Started process (PID=9413) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:15.521+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:32:15.523+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:15.523+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:15.811+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:15.811+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:32:15.814+0000] {processor.py:925} INFO - DAG(s) 'process_list_product_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:15.837+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:15.837+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:32:15.854+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:15.853+0000] {dag.py:4180} INFO - Setting next_dagrun for process_list_product_dag to None, run_after=None
[2025-04-04T20:32:15.889+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.377 seconds
[2025-04-04T20:32:25.465+0000] {processor.py:186} INFO - Started process (PID=9438) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:25.467+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:32:25.469+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:25.468+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:25.755+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:25.755+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:32:25.763+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:25.756+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 292, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:32:25.766+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:25.797+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.345 seconds
[2025-04-04T20:32:55.994+0000] {processor.py:186} INFO - Started process (PID=9504) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:55.995+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:32:55.996+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:55.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:56.278+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:56.277+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:32:56.287+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:32:56.278+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 292, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:32:56.289+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:32:56.326+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.341 seconds
[2025-04-04T20:33:26.476+0000] {processor.py:186} INFO - Started process (PID=9569) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:26.477+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:33:26.479+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:26.478+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:26.744+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:26.744+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:33:26.751+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:26.745+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 292, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:33:26.753+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:26.784+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.316 seconds
[2025-04-04T20:33:57.012+0000] {processor.py:186} INFO - Started process (PID=9632) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:57.013+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:33:57.014+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:57.014+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:57.250+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:57.250+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:33:57.256+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:33:57.251+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 292, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:33:57.259+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:33:57.285+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.281 seconds
[2025-04-04T20:34:27.457+0000] {processor.py:186} INFO - Started process (PID=9697) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:27.458+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:34:27.460+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:27.460+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:27.715+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:27.715+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:34:27.721+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:27.716+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 292, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:34:27.723+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:27.748+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.299 seconds
[2025-04-04T20:34:34.749+0000] {processor.py:186} INFO - Started process (PID=9711) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:34.750+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:34:34.752+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:34.752+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:35.155+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:35.155+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:34:35.158+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:34:35.329+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:35.328+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:34:35.347+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:35.346+0000] {dag.py:3262} INFO - Creating ORM DAG for scrape_amazon_dag
[2025-04-04T20:34:35.349+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:34:35.349+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:34:35.404+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.663 seconds
[2025-04-04T20:35:05.996+0000] {processor.py:186} INFO - Started process (PID=9776) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:05.998+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:35:06.000+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:05.999+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:06.277+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:06.277+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:35:06.281+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:06.299+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:06.298+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:35:06.316+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:06.316+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:35:06.344+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.358 seconds
[2025-04-04T20:35:36.490+0000] {processor.py:186} INFO - Started process (PID=9842) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:36.492+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:35:36.495+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:36.494+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:36.727+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:36.726+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:35:36.729+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:35:36.749+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:36.748+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:35:36.768+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:35:36.768+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:35:36.798+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.317 seconds
[2025-04-04T20:36:06.843+0000] {processor.py:186} INFO - Started process (PID=9907) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:06.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:36:06.846+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:06.845+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:07.106+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:07.105+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:36:07.108+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:07.132+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:07.131+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:36:07.157+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:07.157+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:36:07.190+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T20:36:37.657+0000] {processor.py:186} INFO - Started process (PID=9970) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:37.658+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:36:37.661+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:37.660+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:37.912+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:37.912+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:36:37.915+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:36:37.938+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:37.937+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:36:37.959+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:36:37.959+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:36:37.991+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.342 seconds
[2025-04-04T20:37:08.781+0000] {processor.py:186} INFO - Started process (PID=10035) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:08.783+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:37:08.784+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:08.784+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:09.018+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:09.018+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:37:09.021+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:09.044+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:09.044+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:37:09.068+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:09.068+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:37:09.099+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.325 seconds
[2025-04-04T20:37:39.174+0000] {processor.py:186} INFO - Started process (PID=10100) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:39.176+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:37:39.178+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:39.177+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:39.434+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:39.433+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:37:39.437+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:37:39.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:39.458+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:37:39.476+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:37:39.476+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:37:39.512+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.348 seconds
[2025-04-04T20:38:09.750+0000] {processor.py:186} INFO - Started process (PID=10166) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:09.751+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:38:09.753+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:09.753+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:09.994+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:09.994+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:38:10.031+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:10.052+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:10.051+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:38:10.071+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:10.070+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:38:10.103+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.360 seconds
[2025-04-04T20:38:40.815+0000] {processor.py:186} INFO - Started process (PID=10232) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:40.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:38:40.818+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:40.817+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:41.049+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:41.048+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:38:41.051+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:38:41.074+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:41.073+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:38:41.094+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:38:41.094+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:38:41.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.322 seconds
[2025-04-04T20:39:11.347+0000] {processor.py:186} INFO - Started process (PID=10297) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:11.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:39:11.350+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:11.350+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:11.625+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:11.624+0000] {test_crawl_list_product.py:246} ERROR - Error reading CSV file: CSV file must contain 'Department', 'Sub Department', and 'URL' columns
[2025-04-04T20:39:11.627+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:11.651+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:11.650+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:39:11.667+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:11.667+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:39:11.700+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.360 seconds
[2025-04-04T20:39:33.016+0000] {processor.py:186} INFO - Started process (PID=10348) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:33.018+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:39:33.020+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:33.019+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:33.311+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:33.292+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:39:33.312+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:33.338+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.332 seconds
[2025-04-04T20:39:55.112+0000] {processor.py:186} INFO - Started process (PID=10403) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:55.113+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:39:55.115+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:55.115+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:55.413+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:39:55.406+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:39:55.414+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:39:55.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.341 seconds
[2025-04-04T20:40:25.909+0000] {processor.py:186} INFO - Started process (PID=10464) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:25.910+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:40:25.912+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:40:25.911+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:26.247+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:40:26.241+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:40:26.248+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:26.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.375 seconds
[2025-04-04T20:40:56.619+0000] {processor.py:186} INFO - Started process (PID=10529) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:56.621+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:40:56.623+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:40:56.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:56.909+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:40:56.901+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:40:56.910+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:40:56.934+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.326 seconds
[2025-04-04T20:41:27.007+0000] {processor.py:186} INFO - Started process (PID=10595) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:27.009+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:41:27.010+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:41:27.010+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:27.298+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:41:27.292+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:41:27.299+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:27.525+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.527 seconds
[2025-04-04T20:41:57.713+0000] {processor.py:186} INFO - Started process (PID=10662) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:57.714+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:41:57.716+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:41:57.715+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:58.014+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:41:58.005+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:41:58.015+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:41:58.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T20:42:28.232+0000] {processor.py:186} INFO - Started process (PID=10724) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:28.234+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:42:28.236+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:42:28.236+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:28.550+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:42:28.543+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:42:28.552+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:28.583+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.360 seconds
[2025-04-04T20:42:59.061+0000] {processor.py:186} INFO - Started process (PID=10789) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:59.063+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:42:59.064+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:42:59.064+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:59.332+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:42:59.324+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:42:59.334+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:42:59.359+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.308 seconds
[2025-04-04T20:43:08.583+0000] {processor.py:186} INFO - Started process (PID=10805) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:08.584+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:43:08.586+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:43:08.585+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:09.113+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:43:09.101+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:43:09.115+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:09.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.568 seconds
[2025-04-04T20:43:39.446+0000] {processor.py:186} INFO - Started process (PID=10873) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:39.448+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:43:39.449+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:43:39.449+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:39.967+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:43:39.960+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:43:39.968+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:43:39.993+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.554 seconds
[2025-04-04T20:44:10.860+0000] {processor.py:186} INFO - Started process (PID=10938) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:10.862+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:44:10.863+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:10.863+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:11.304+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:11.296+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:44:11.305+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:11.331+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.479 seconds
[2025-04-04T20:44:21.209+0000] {processor.py:186} INFO - Started process (PID=10953) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:21.210+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:44:21.211+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:21.211+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:21.693+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:21.684+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:44:21.694+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:21.721+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.522 seconds
[2025-04-04T20:44:52.078+0000] {processor.py:186} INFO - Started process (PID=11021) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:52.079+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:44:52.081+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:52.080+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:52.550+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:44:52.543+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:44:52.551+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:44:52.578+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.509 seconds
[2025-04-04T20:45:22.692+0000] {processor.py:186} INFO - Started process (PID=11089) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:22.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:45:22.695+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:45:22.694+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:23.100+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:45:23.092+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:45:23.101+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:23.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.445 seconds
[2025-04-04T20:45:53.680+0000] {processor.py:186} INFO - Started process (PID=11162) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:53.681+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:45:53.683+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:45:53.682+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:54.264+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:45:54.256+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:45:54.266+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:45:54.299+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.627 seconds
[2025-04-04T20:46:24.558+0000] {processor.py:186} INFO - Started process (PID=11227) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:24.560+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:46:24.562+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:46:24.561+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:25.000+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:46:24.995+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:46:25.001+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:25.025+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.476 seconds
[2025-04-04T20:46:55.402+0000] {processor.py:186} INFO - Started process (PID=11291) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:55.403+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:46:55.405+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:46:55.404+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:55.879+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:46:55.872+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:46:55.880+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:46:55.904+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.510 seconds
[2025-04-04T20:47:26.168+0000] {processor.py:186} INFO - Started process (PID=11354) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:26.169+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:47:26.170+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:47:26.170+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:26.702+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:47:26.696+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:47:26.704+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:26.725+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.567 seconds
[2025-04-04T20:47:45.211+0000] {processor.py:186} INFO - Started process (PID=11401) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:45.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:47:45.214+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:47:45.214+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:45.843+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:47:45.836+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:47:45.845+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:47:45.871+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.672 seconds
[2025-04-04T20:48:16.551+0000] {processor.py:186} INFO - Started process (PID=11466) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:16.552+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:48:16.554+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:16.553+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:17.026+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:17.020+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 280, in <module>
    department, sub_department, url = row['Department'], row['Sub Department'], row['URL']
                                                         ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:48:17.028+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:17.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.523 seconds
[2025-04-04T20:48:25.618+0000] {processor.py:186} INFO - Started process (PID=11482) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:25.625+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:48:25.646+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:25.645+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:26.426+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.426+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.431+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.430+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.432+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.432+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.435+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.434+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.438+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.438+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.444+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.443+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.445+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.445+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.449+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.447+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.451+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.451+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.455+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.454+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.456+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.456+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.457+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.460+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.460+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.463+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.462+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.465+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.465+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.468+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.467+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:26.564+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:26.468+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 296, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:48:26.574+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:26.617+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.010 seconds
[2025-04-04T20:48:49.347+0000] {processor.py:186} INFO - Started process (PID=11535) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:49.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:48:49.352+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.351+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:49.856+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.856+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.857+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.857+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.858+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.857+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.858+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.858+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.859+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.859+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.860+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.860+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.861+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.860+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.862+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.861+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.862+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.862+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.863+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.863+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.864+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.863+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.865+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.864+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.865+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.865+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.866+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.866+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.867+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.867+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.868+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.868+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:48:49.870+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:48:49.994+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:49.994+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:48:50.008+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:48:50.008+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:48:50.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.710 seconds
[2025-04-04T20:49:20.346+0000] {processor.py:186} INFO - Started process (PID=11601) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:20.347+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:49:20.349+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.349+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:20.759+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.759+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.760+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.760+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.761+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.761+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.762+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.762+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.763+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.762+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.763+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.763+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.764+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.764+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.765+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.765+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.765+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.765+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.766+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.766+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.767+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.767+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.767+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.767+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.768+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.768+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.769+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.769+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.769+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.769+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.770+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.770+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:20.772+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:20.790+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.789+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:49:20.804+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:20.804+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:49:20.835+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.496 seconds
[2025-04-04T20:49:51.182+0000] {processor.py:186} INFO - Started process (PID=11666) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:51.183+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:49:51.184+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.184+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:51.769+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.769+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.772+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.771+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.778+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.777+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.781+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.780+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.783+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.782+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.784+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.784+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.786+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.785+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.795+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.794+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.797+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.797+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.801+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.800+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.803+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.802+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.808+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.804+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.810+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.810+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.814+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.813+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.818+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.818+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.821+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.821+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:49:51.829+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:49:51.867+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.866+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:49:51.902+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:49:51.901+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:49:51.936+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.762 seconds
[2025-04-04T20:50:22.110+0000] {processor.py:186} INFO - Started process (PID=11732) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:22.111+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:50:22.112+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.112+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:22.600+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.600+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.602+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.602+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.603+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.603+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.605+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.604+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.606+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.606+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.607+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.607+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.608+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.608+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.610+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.609+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.611+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.611+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.612+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.612+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.614+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.614+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.615+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.615+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.617+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.616+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.618+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.617+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.619+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.618+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.620+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.620+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:22.623+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:22.646+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.645+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:50:22.667+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:22.666+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:50:22.705+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.603 seconds
[2025-04-04T20:50:53.062+0000] {processor.py:186} INFO - Started process (PID=11799) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:53.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:50:53.075+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.074+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:53.557+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.556+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.558+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.558+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.559+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.559+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.560+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.560+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.562+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.561+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.563+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.563+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.566+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.565+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.567+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.567+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.574+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.573+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.577+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.577+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.580+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.579+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.581+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.581+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.582+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.582+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.584+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.583+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.585+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.585+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.586+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.586+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:50:53.590+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:50:53.614+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.613+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:50:53.631+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:50:53.631+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:50:53.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.619 seconds
[2025-04-04T20:51:23.939+0000] {processor.py:186} INFO - Started process (PID=11865) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:23.940+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:51:23.941+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:23.941+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:24.158+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.158+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.159+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.159+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.160+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.160+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.160+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.160+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.161+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.161+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.162+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.162+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.163+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.162+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.163+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.163+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.164+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.164+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.165+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.165+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.166+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.165+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.166+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.166+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.167+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.167+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.168+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.168+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.168+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.168+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.170+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.169+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:24.172+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:24.191+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.191+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:51:24.207+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:24.207+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:51:24.235+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.303 seconds
[2025-04-04T20:51:54.312+0000] {processor.py:186} INFO - Started process (PID=11930) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:54.313+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:51:54.314+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.314+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:54.624+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.624+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.625+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.625+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.626+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.626+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.626+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.626+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.627+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.627+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.628+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.627+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.628+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.628+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.629+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.629+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.629+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.629+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.630+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.630+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.631+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.630+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.631+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.631+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.632+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.632+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.632+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.632+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.633+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.633+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.633+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.633+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:51:54.636+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:51:54.654+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.654+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:51:54.669+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:51:54.669+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:51:54.698+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.394 seconds
[2025-04-04T20:52:17.880+0000] {processor.py:186} INFO - Started process (PID=11981) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:17.881+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:52:17.883+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:17.882+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:18.284+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.283+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.286+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.285+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.288+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.287+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.291+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.290+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.294+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.293+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.296+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.295+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.299+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.299+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.302+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.301+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.304+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.303+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.308+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.307+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.311+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.310+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.315+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.315+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.323+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.322+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.328+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.328+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.330+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.330+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.338+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.336+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:18.376+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:18.423+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.422+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T20:52:18.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:18.458+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T20:52:18.513+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.640 seconds
[2025-04-04T20:52:31.677+0000] {processor.py:186} INFO - Started process (PID=12008) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:31.679+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:52:31.680+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.680+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:31.975+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.974+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.976+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.976+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.977+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.977+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.979+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.978+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.980+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.980+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.981+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.980+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.981+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.981+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.982+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.982+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.983+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.983+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.984+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.984+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.985+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.985+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.986+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.985+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.986+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.986+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.987+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.987+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.988+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.988+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.989+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.989+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:52:31.998+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:52:31.990+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:52:32.001+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:52:32.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.367 seconds
[2025-04-04T20:53:02.102+0000] {processor.py:186} INFO - Started process (PID=12073) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:02.103+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:53:02.105+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.105+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:02.358+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.357+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.359+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.358+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.360+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.359+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.360+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.360+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.361+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.361+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.362+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.362+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.363+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.363+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.363+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.363+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.364+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.364+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.365+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.365+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.365+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.365+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.366+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.366+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.367+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.367+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.367+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.367+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.368+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.368+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.369+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.369+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:02.374+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:02.369+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:53:02.376+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:02.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.311 seconds
[2025-04-04T20:53:03.256+0000] {processor.py:186} INFO - Started process (PID=12074) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:03.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:53:03.259+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.258+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:03.731+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.730+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.732+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.732+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.733+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.733+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.736+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.735+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.738+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.738+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.739+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.739+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.740+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.740+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.741+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.741+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.742+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.742+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.742+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.742+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.743+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.743+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.744+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.744+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.744+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.744+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.745+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.745+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.747+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.746+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.748+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.748+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:03.755+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:03.749+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:53:03.757+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:03.790+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.547 seconds
[2025-04-04T20:53:33.066+0000] {processor.py:186} INFO - Started process (PID=12139) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:33.068+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:53:33.073+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.072+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:33.435+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.434+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.442+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.441+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.443+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.443+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.444+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.443+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.444+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.444+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.446+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.445+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.447+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.447+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.449+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.448+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.450+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.450+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.451+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.451+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.455+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.454+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.457+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.457+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.458+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.459+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.459+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.460+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.460+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.461+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.461+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:33.468+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:33.462+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:53:33.473+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:33.534+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.482 seconds
[2025-04-04T20:53:50.931+0000] {processor.py:186} INFO - Started process (PID=12181) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:50.933+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:53:50.934+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:50.934+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:51.210+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.209+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.211+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.211+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.212+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.212+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.213+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.213+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.214+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.214+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.215+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.215+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.216+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.215+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.217+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.216+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.218+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.218+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.219+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.219+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.220+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.220+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.221+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.221+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.222+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.222+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.223+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.223+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.224+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.224+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.225+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.225+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:53:51.245+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:53:51.225+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:53:51.247+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:53:51.277+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T20:54:21.993+0000] {processor.py:186} INFO - Started process (PID=12246) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:21.995+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:54:21.996+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:21.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:22.275+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.274+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.276+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.275+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.276+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.276+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.277+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.277+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.278+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.278+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.279+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.279+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.280+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.279+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.280+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.280+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.281+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.281+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.282+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.282+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.283+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.282+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.283+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.283+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.284+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.284+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.285+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.285+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.286+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.286+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.287+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.286+0000] {test_crawl_list_product.py:282} ERROR - Column not found: 'Sub Department'
[2025-04-04T20:54:22.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:22.287+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 295, in <module>
    task
NameError: name 'task' is not defined
[2025-04-04T20:54:22.296+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:22.323+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.374 seconds
[2025-04-04T20:54:34.738+0000] {processor.py:186} INFO - Started process (PID=12270) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:34.739+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:54:34.741+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:34.741+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:35.029+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:54:35.022+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:54:35.030+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:54:35.056+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.330 seconds
[2025-04-04T20:55:05.278+0000] {processor.py:186} INFO - Started process (PID=12337) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:05.279+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:55:05.281+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:55:05.280+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:05.576+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:55:05.569+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:55:05.578+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:05.616+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.348 seconds
[2025-04-04T20:55:35.975+0000] {processor.py:186} INFO - Started process (PID=12400) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:35.976+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:55:35.978+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:55:35.977+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:36.255+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:55:36.248+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:55:36.257+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:55:36.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.318 seconds
[2025-04-04T20:56:06.393+0000] {processor.py:186} INFO - Started process (PID=12469) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:06.394+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:56:06.396+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:56:06.396+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:06.650+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:56:06.643+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:56:06.651+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:06.680+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.296 seconds
[2025-04-04T20:56:36.742+0000] {processor.py:186} INFO - Started process (PID=12532) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:36.743+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:56:36.745+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:56:36.745+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:37.045+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:56:37.036+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:56:37.047+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:56:37.076+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.344 seconds
[2025-04-04T20:57:07.160+0000] {processor.py:186} INFO - Started process (PID=12599) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:07.161+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:57:07.163+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:57:07.162+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:07.443+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:57:07.436+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:57:07.444+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:07.480+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.331 seconds
[2025-04-04T20:57:37.621+0000] {processor.py:186} INFO - Started process (PID=12665) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:37.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:57:37.625+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:57:37.624+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:37.891+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:57:37.884+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:57:37.892+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:57:37.922+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.309 seconds
[2025-04-04T20:58:08.436+0000] {processor.py:186} INFO - Started process (PID=12729) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:08.437+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:58:08.439+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:58:08.439+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:08.767+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:58:08.761+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department=row['Sub Department']
                   ~~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'Sub Department'
[2025-04-04T20:58:08.768+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:08.794+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.367 seconds
[2025-04-04T20:58:31.506+0000] {processor.py:186} INFO - Started process (PID=12781) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:31.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:58:31.511+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:58:31.511+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:31.809+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:58:31.801+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ' Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department = row[' Sub Department']
                     ~~~^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: ' Sub Department'
[2025-04-04T20:58:31.810+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:58:31.833+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.336 seconds
[2025-04-04T20:59:02.413+0000] {processor.py:186} INFO - Started process (PID=12847) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:02.414+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:59:02.416+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:59:02.416+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:02.670+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:59:02.662+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ' Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department = row[' Sub Department']
                     ~~~^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: ' Sub Department'
[2025-04-04T20:59:02.671+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:02.702+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.298 seconds
[2025-04-04T20:59:32.806+0000] {processor.py:186} INFO - Started process (PID=12916) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:32.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T20:59:32.810+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:59:32.810+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:33.069+0000] {logging_mixin.py:190} INFO - [2025-04-04T20:59:33.063+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ' Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department = row[' Sub Department']
                     ~~~^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: ' Sub Department'
[2025-04-04T20:59:33.070+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T20:59:33.100+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.301 seconds
[2025-04-04T21:00:03.344+0000] {processor.py:186} INFO - Started process (PID=12981) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:03.345+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:00:03.347+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:03.346+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:03.643+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:03.635+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ' Sub Department'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 279, in <module>
    sub_department = row[' Sub Department']
                     ~~~^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1040, in __getitem__
    return self._get_value(key)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/series.py", line 1156, in _get_value
    loc = self.index.get_loc(label)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: ' Sub Department'
[2025-04-04T21:00:03.645+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:03.676+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.341 seconds
[2025-04-04T21:00:24.919+0000] {processor.py:186} INFO - Started process (PID=13025) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:24.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:00:24.922+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:24.922+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:25.202+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:25.191+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/test_crawl_list_product.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/test_crawl_list_product.py", line 284, in <module>
    python_callable=process_row,
                    ^^^^^^^^^^^
NameError: name 'process_row' is not defined
[2025-04-04T21:00:25.212+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:25.241+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.331 seconds
[2025-04-04T21:00:27.677+0000] {processor.py:186} INFO - Started process (PID=13038) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:27.678+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:00:27.680+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:27.680+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:28.105+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:28.301+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:28.300+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:00:28.316+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:28.316+0000] {dag.py:3262} INFO - Creating ORM DAG for scrape_amazon_dag
[2025-04-04T21:00:28.317+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:28.317+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:00:28.357+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.692 seconds
[2025-04-04T21:00:58.635+0000] {processor.py:186} INFO - Started process (PID=13102) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:58.636+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:00:58.638+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:58.638+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:58.970+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:00:59.015+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:59.014+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:00:59.033+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:00:59.032+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:00:59.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.439 seconds
[2025-04-04T21:01:23.164+0000] {processor.py:186} INFO - Started process (PID=13152) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:23.166+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:01:23.167+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:23.167+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:23.601+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:23.849+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:23.848+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:01:23.868+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:23.868+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:01:23.905+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.750 seconds
[2025-04-04T21:01:54.734+0000] {processor.py:186} INFO - Started process (PID=13217) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:54.735+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:01:54.737+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:54.737+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:55.043+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:01:55.087+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:55.086+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:01:55.108+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:01:55.108+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:01:55.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.423 seconds
[2025-04-04T21:02:25.669+0000] {processor.py:186} INFO - Started process (PID=13282) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:25.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:02:25.674+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:25.673+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:25.950+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:25.989+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:25.989+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:02:26.010+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:26.010+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:02:26.042+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.386 seconds
[2025-04-04T21:02:56.148+0000] {processor.py:186} INFO - Started process (PID=13347) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:56.149+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:02:56.151+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:56.150+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:56.470+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:02:56.510+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:56.510+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:02:56.526+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:02:56.526+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:02:56.556+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.417 seconds
[2025-04-04T21:03:27.038+0000] {processor.py:186} INFO - Started process (PID=13417) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:27.040+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:03:27.041+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:27.041+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:27.307+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:27.351+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:27.350+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:03:27.369+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:27.369+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:03:27.399+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.370 seconds
[2025-04-04T21:03:57.547+0000] {processor.py:186} INFO - Started process (PID=13482) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:57.548+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:03:57.550+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:57.550+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:57.807+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:03:57.852+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:57.852+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:03:57.871+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:03:57.871+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:03:57.904+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.369 seconds
[2025-04-04T21:04:03.458+0000] {processor.py:186} INFO - Started process (PID=13493) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:03.460+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:04:03.462+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:03.461+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:03.745+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:03.787+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:03.786+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:04:03.806+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:03.806+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:04:03.850+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.399 seconds
[2025-04-04T21:04:17.505+0000] {processor.py:186} INFO - Started process (PID=13524) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:17.507+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:04:17.509+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:17.508+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:17.886+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:18.133+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:18.133+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:04:18.156+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:18.156+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:04:18.205+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.712 seconds
[2025-04-04T21:04:48.718+0000] {processor.py:186} INFO - Started process (PID=13590) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:48.719+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:04:48.722+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:48.721+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:49.105+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:04:49.139+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:49.138+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:04:49.158+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:04:49.158+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:04:49.187+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.481 seconds
[2025-04-04T21:05:19.488+0000] {processor.py:186} INFO - Started process (PID=13655) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:19.491+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:05:19.493+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:19.493+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:19.931+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:20.132+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:20.131+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:05:20.157+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:20.156+0000] {dag.py:3262} INFO - Creating ORM DAG for scrape_amazon_dag
[2025-04-04T21:05:20.158+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:20.158+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:05:20.692+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.221 seconds
[2025-04-04T21:05:51.044+0000] {processor.py:186} INFO - Started process (PID=13722) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:51.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:05:51.048+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:51.048+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:51.318+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:05:51.346+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:51.346+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:05:51.363+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:05:51.363+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:05:51.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.364 seconds
[2025-04-04T21:06:21.493+0000] {processor.py:186} INFO - Started process (PID=13791) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:21.494+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:06:21.496+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:21.496+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:21.792+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:21.819+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:21.819+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:06:21.837+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:21.836+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:06:21.887+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.406 seconds
[2025-04-04T21:06:52.239+0000] {processor.py:186} INFO - Started process (PID=13854) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:52.241+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:06:52.243+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:52.243+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:52.582+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:06:52.611+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:52.610+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:06:52.627+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:06:52.626+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:06:52.660+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.431 seconds
[2025-04-04T21:07:22.961+0000] {processor.py:186} INFO - Started process (PID=13918) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:22.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:07:22.965+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:22.964+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:23.266+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:23.303+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:23.302+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:07:23.321+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:23.320+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:07:23.348+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.397 seconds
[2025-04-04T21:07:53.443+0000] {processor.py:186} INFO - Started process (PID=13985) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:53.444+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:07:53.445+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:53.445+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:53.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:07:53.760+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:53.760+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:07:53.781+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:07:53.781+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:07:53.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.380 seconds
[2025-04-04T21:08:24.272+0000] {processor.py:186} INFO - Started process (PID=14047) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:24.273+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:08:24.277+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:24.276+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:24.585+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:24.624+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:24.624+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:08:24.647+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:24.646+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:08:24.681+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.420 seconds
[2025-04-04T21:08:55.295+0000] {processor.py:186} INFO - Started process (PID=14112) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:55.297+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:08:55.298+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:55.298+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:55.581+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:08:55.624+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:55.623+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:08:55.647+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:08:55.647+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:08:55.684+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.398 seconds
[2025-04-04T21:09:26.347+0000] {processor.py:186} INFO - Started process (PID=14176) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:26.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:09:26.350+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:26.349+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:26.666+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:26.703+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:26.702+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:09:26.730+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:26.729+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:09:26.767+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.464 seconds
[2025-04-04T21:09:57.340+0000] {processor.py:186} INFO - Started process (PID=14243) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:57.341+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:09:57.342+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:57.342+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:57.633+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:09:57.663+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:57.662+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:09:57.685+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:09:57.684+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:09:57.714+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.382 seconds
[2025-04-04T21:10:27.888+0000] {processor.py:186} INFO - Started process (PID=14316) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:27.889+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:10:27.890+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:27.890+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:28.157+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:28.313+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:28.312+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:10:28.329+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:28.329+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:10:28.362+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.482 seconds
[2025-04-04T21:10:59.382+0000] {processor.py:186} INFO - Started process (PID=14383) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:59.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:10:59.386+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:59.385+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:59.767+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:10:59.850+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:59.850+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:10:59.870+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:10:59.870+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:10:59.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.531 seconds
[2025-04-04T21:11:30.068+0000] {processor.py:186} INFO - Started process (PID=14450) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:30.069+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:11:30.072+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:30.071+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:30.407+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:30.442+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:30.442+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:11:30.462+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:30.462+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:11:30.494+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.440 seconds
[2025-04-04T21:11:51.506+0000] {processor.py:186} INFO - Started process (PID=14483) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:51.507+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:11:51.509+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:51.509+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:51.758+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:11:51.799+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:51.798+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:11:51.851+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:11:51.851+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:11:51.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.412 seconds
[2025-04-04T21:12:11.347+0000] {processor.py:186} INFO - Started process (PID=14527) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:11.349+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:12:11.350+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:11.350+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:11.649+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:11.683+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:11.682+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:12:11.705+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:11.705+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:12:11.754+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.414 seconds
[2025-04-04T21:12:12.773+0000] {processor.py:186} INFO - Started process (PID=14528) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:12.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:12:12.777+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:12.777+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:13.084+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:13.117+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:13.116+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:12:13.143+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:13.143+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:12:13.181+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.419 seconds
[2025-04-04T21:12:43.403+0000] {processor.py:186} INFO - Started process (PID=14595) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:43.404+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:12:43.406+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:43.405+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:43.693+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:12:43.735+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:43.734+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:12:43.752+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:12:43.752+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:12:43.775+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.382 seconds
[2025-04-04T21:13:13.962+0000] {processor.py:186} INFO - Started process (PID=14662) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:13.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:13:13.964+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:13.964+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:14.201+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:14.232+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:14.231+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:13:14.252+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:14.252+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:13:14.283+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.329 seconds
[2025-04-04T21:13:45.079+0000] {processor.py:186} INFO - Started process (PID=14726) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:45.081+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:13:45.082+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:45.082+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:45.373+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:45.409+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:45.408+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:13:45.431+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:45.430+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:13:45.465+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.394 seconds
[2025-04-04T21:13:55.558+0000] {processor.py:186} INFO - Started process (PID=14745) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:55.560+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:13:55.562+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:55.562+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:55.997+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:13:56.046+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:56.045+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:13:56.065+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:13:56.065+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:13:56.100+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.553 seconds
[2025-04-04T21:14:28.287+0000] {processor.py:186} INFO - Started process (PID=14810) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:28.288+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:14:28.292+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:28.291+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:28.705+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:28.752+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:28.752+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:14:28.784+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:28.784+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:14:28.813+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.538 seconds
[2025-04-04T21:14:59.424+0000] {processor.py:186} INFO - Started process (PID=14881) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:59.426+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:14:59.428+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:59.428+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:59.705+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:14:59.744+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:59.743+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:14:59.766+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:14:59.766+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:14:59.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.390 seconds
[2025-04-04T21:15:30.059+0000] {processor.py:186} INFO - Started process (PID=14949) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:15:30.060+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:15:30.061+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:15:30.061+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:15:30.369+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:15:30.431+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:15:30.431+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:15:30.448+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:15:30.447+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:15:30.495+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.444 seconds
[2025-04-04T21:16:01.462+0000] {processor.py:186} INFO - Started process (PID=15014) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:01.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:16:01.466+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:01.465+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:02.167+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:02.215+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:02.214+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:16:02.239+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:02.238+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:16:02.272+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.819 seconds
[2025-04-04T21:16:32.826+0000] {processor.py:186} INFO - Started process (PID=15080) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:32.827+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:16:32.828+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:32.828+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:33.096+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:16:33.139+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:33.138+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:16:33.163+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:16:33.163+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:16:33.198+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.380 seconds
[2025-04-04T21:17:03.285+0000] {processor.py:186} INFO - Started process (PID=15145) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:03.287+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:17:03.290+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:03.289+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:03.556+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:03.589+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:03.588+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:17:03.608+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:03.608+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:17:03.639+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.365 seconds
[2025-04-04T21:17:33.718+0000] {processor.py:186} INFO - Started process (PID=15211) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:33.719+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:17:33.721+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:33.721+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:34.024+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:17:34.068+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:34.067+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:17:34.089+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:17:34.089+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:17:34.114+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.403 seconds
[2025-04-04T21:18:04.888+0000] {processor.py:186} INFO - Started process (PID=15276) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:04.889+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:18:04.892+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:04.891+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:05.196+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:05.230+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:05.229+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:18:05.251+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:05.251+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:18:05.278+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.398 seconds
[2025-04-04T21:18:36.226+0000] {processor.py:186} INFO - Started process (PID=15342) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:36.228+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:18:36.230+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:36.230+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:36.515+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:18:36.546+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:36.545+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:18:36.564+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:18:36.564+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:18:36.837+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.621 seconds
[2025-04-04T21:19:07.028+0000] {processor.py:186} INFO - Started process (PID=15407) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:07.029+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:19:07.031+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:07.031+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:07.261+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:07.291+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:07.290+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:19:07.311+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:07.311+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:19:07.344+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.324 seconds
[2025-04-04T21:19:37.419+0000] {processor.py:186} INFO - Started process (PID=15473) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:37.420+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:19:37.422+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:37.422+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:37.670+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:19:37.701+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:37.700+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:19:37.717+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:19:37.717+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:19:37.749+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.337 seconds
[2025-04-04T21:20:08.673+0000] {processor.py:186} INFO - Started process (PID=15539) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:08.674+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:20:08.675+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:08.675+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:09.095+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:09.121+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:09.120+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:20:09.135+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:09.134+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:20:09.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.496 seconds
[2025-04-04T21:20:39.344+0000] {processor.py:186} INFO - Started process (PID=15605) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:39.345+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:20:39.347+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:39.347+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:39.917+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:20:39.974+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:39.974+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:20:39.999+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:20:39.998+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:20:40.042+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.708 seconds
[2025-04-04T21:21:10.527+0000] {processor.py:186} INFO - Started process (PID=15672) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:10.529+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:21:10.531+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:10.530+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:11.294+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:11.349+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:11.346+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:21:11.391+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:11.390+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:21:11.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.928 seconds
[2025-04-04T21:21:42.069+0000] {processor.py:186} INFO - Started process (PID=15738) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:42.070+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:21:42.071+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:42.071+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:42.539+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:21:42.565+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:42.565+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:21:42.580+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:21:42.580+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:21:42.773+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.714 seconds
[2025-04-04T21:22:12.972+0000] {processor.py:186} INFO - Started process (PID=15805) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:12.973+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:22:12.975+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:12.975+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:13.424+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:13.454+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:13.453+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:22:13.470+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:13.470+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:22:13.494+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.529 seconds
[2025-04-04T21:22:43.552+0000] {processor.py:186} INFO - Started process (PID=15871) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:43.553+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:22:43.555+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:43.554+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:44.066+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:22:44.093+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:44.093+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:22:44.109+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:22:44.108+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:22:44.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.594 seconds
[2025-04-04T21:23:14.467+0000] {processor.py:186} INFO - Started process (PID=15939) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:14.469+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:23:14.472+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:14.470+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:14.961+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:14.991+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:14.990+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:23:15.008+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:15.008+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:23:15.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.586 seconds
[2025-04-04T21:23:45.313+0000] {processor.py:186} INFO - Started process (PID=16004) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:45.316+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:23:45.319+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:45.318+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:47.144+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:23:47.173+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:47.173+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:23:47.190+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:23:47.190+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:23:47.221+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.929 seconds
[2025-04-04T21:24:17.635+0000] {processor.py:186} INFO - Started process (PID=16070) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:17.637+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:24:17.640+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:17.639+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:18.159+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:18.194+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:18.193+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:24:18.208+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:18.208+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:24:18.237+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.615 seconds
[2025-04-04T21:24:48.561+0000] {processor.py:186} INFO - Started process (PID=16135) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:48.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:24:48.566+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:48.565+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:48.911+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:24:48.960+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:48.959+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:24:48.994+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:24:48.994+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:24:49.390+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.846 seconds
[2025-04-04T21:25:19.616+0000] {processor.py:186} INFO - Started process (PID=16201) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:19.618+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:25:19.620+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:19.620+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:20.491+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:20.573+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:20.572+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:25:20.606+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:20.606+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:25:20.700+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.102 seconds
[2025-04-04T21:25:50.987+0000] {processor.py:186} INFO - Started process (PID=16269) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:50.988+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:25:50.990+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:50.989+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:51.284+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:25:51.317+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:51.316+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:25:51.335+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:25:51.335+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:25:51.443+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.463 seconds
[2025-04-04T21:26:21.560+0000] {processor.py:186} INFO - Started process (PID=16336) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:21.561+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:26:21.563+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:21.562+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:21.878+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:21.914+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:21.914+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:26:21.934+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:21.934+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:26:21.968+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.418 seconds
[2025-04-04T21:26:52.863+0000] {processor.py:186} INFO - Started process (PID=16403) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:52.865+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:26:52.866+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:52.866+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:53.135+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:26:53.165+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:53.165+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:26:53.183+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:26:53.183+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:26:53.214+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.361 seconds
[2025-04-04T21:27:23.600+0000] {processor.py:186} INFO - Started process (PID=16469) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:23.601+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:27:23.602+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:23.602+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:23.882+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:23.958+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:23.957+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:27:23.993+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:23.993+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:27:24.049+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.459 seconds
[2025-04-04T21:27:54.377+0000] {processor.py:186} INFO - Started process (PID=16535) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:54.378+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:27:54.380+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:54.380+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:54.656+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:27:54.698+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:54.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:27:54.728+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:27:54.728+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:27:54.772+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.404 seconds
[2025-04-04T21:28:24.886+0000] {processor.py:186} INFO - Started process (PID=16601) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:24.888+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:28:24.891+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:24.890+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:25.187+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:25.224+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:25.223+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:28:25.244+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:25.244+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:28:25.274+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.402 seconds
[2025-04-04T21:28:55.509+0000] {processor.py:186} INFO - Started process (PID=16669) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:55.510+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:28:55.513+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:55.512+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:55.814+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:28:55.846+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:55.845+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:28:55.865+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:28:55.865+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:28:55.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.399 seconds
[2025-04-04T21:29:26.066+0000] {processor.py:186} INFO - Started process (PID=16735) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:26.068+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:29:26.070+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:26.069+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:26.334+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:26.363+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:26.363+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:29:26.380+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:26.380+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:29:26.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.356 seconds
[2025-04-04T21:29:56.622+0000] {processor.py:186} INFO - Started process (PID=16798) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:56.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:29:56.625+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:56.625+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:56.924+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:29:56.959+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:56.958+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:29:56.979+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:29:56.979+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:29:57.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.397 seconds
[2025-04-04T21:30:27.227+0000] {processor.py:186} INFO - Started process (PID=16864) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:27.228+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:30:27.230+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:27.229+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:27.517+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:27.555+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:27.554+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:30:27.577+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:27.577+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:30:27.605+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.387 seconds
[2025-04-04T21:30:57.822+0000] {processor.py:186} INFO - Started process (PID=16930) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:57.824+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:30:57.825+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:57.825+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:58.335+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:30:58.375+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:58.374+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:30:58.409+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:30:58.408+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:30:58.475+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.654 seconds
[2025-04-04T21:31:28.822+0000] {processor.py:186} INFO - Started process (PID=16997) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:28.823+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:31:28.824+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:31:28.824+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:29.120+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:29.171+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:31:29.171+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:31:29.193+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:31:29.193+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:31:29.232+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.418 seconds
[2025-04-04T21:31:59.613+0000] {processor.py:186} INFO - Started process (PID=17058) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:59.615+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:31:59.617+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:31:59.617+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:59.946+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:31:59.981+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:31:59.981+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:32:00.006+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:32:00.006+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:32:00.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.445 seconds
[2025-04-04T21:32:30.285+0000] {processor.py:186} INFO - Started process (PID=17123) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:32:30.287+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:32:30.290+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:32:30.289+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:32:31.686+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:32:31.718+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:32:31.717+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:32:31.735+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:32:31.735+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:32:31.767+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.493 seconds
[2025-04-04T21:33:01.882+0000] {processor.py:186} INFO - Started process (PID=17192) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:01.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:33:01.885+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:01.884+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:02.131+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:02.164+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:02.164+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:33:02.187+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:02.186+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:33:02.217+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.343 seconds
[2025-04-04T21:33:32.382+0000] {processor.py:186} INFO - Started process (PID=17258) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:32.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:33:32.385+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:32.385+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:32.670+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:33:32.700+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:32.699+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:33:32.717+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:33:32.716+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:33:32.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.371 seconds
[2025-04-04T21:34:02.856+0000] {processor.py:186} INFO - Started process (PID=17330) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:02.858+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:34:02.860+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:02.859+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:03.130+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:03.177+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:03.176+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:34:03.195+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:03.195+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:34:03.228+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.380 seconds
[2025-04-04T21:34:33.459+0000] {processor.py:186} INFO - Started process (PID=17396) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:33.460+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:34:33.462+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:33.461+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:33.711+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:34:33.744+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:33.743+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:34:33.763+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:34:33.763+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:34:33.795+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.344 seconds
[2025-04-04T21:35:03.913+0000] {processor.py:186} INFO - Started process (PID=17463) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:03.915+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:35:03.918+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:03.917+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:04.176+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:04.205+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:04.205+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:35:04.226+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:04.225+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:35:04.260+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.357 seconds
[2025-04-04T21:35:34.572+0000] {processor.py:186} INFO - Started process (PID=17531) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:34.573+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:35:34.575+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:34.575+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:34.844+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:35:34.872+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:34.872+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:35:34.889+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:35:34.888+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:35:34.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.351 seconds
[2025-04-04T21:36:05.011+0000] {processor.py:186} INFO - Started process (PID=17597) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:05.012+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:36:05.015+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:05.014+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:05.311+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:05.346+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:05.345+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:36:05.367+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:05.366+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:36:05.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.398 seconds
[2025-04-04T21:36:35.606+0000] {processor.py:186} INFO - Started process (PID=17659) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:35.607+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:36:35.608+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:35.608+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:35.948+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:36:36.018+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:36.017+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:36:36.037+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:36:36.037+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:36:36.083+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.488 seconds
[2025-04-04T21:37:06.270+0000] {processor.py:186} INFO - Started process (PID=17725) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:06.271+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:37:06.273+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:06.273+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:06.634+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:06.668+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:06.667+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:37:06.686+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:06.686+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:37:06.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.458 seconds
[2025-04-04T21:37:36.899+0000] {processor.py:186} INFO - Started process (PID=17792) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:36.900+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:37:36.902+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:36.901+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:37.166+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:37:37.196+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:37.196+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:37:37.217+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:37:37.217+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:37:37.242+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.351 seconds
[2025-04-04T21:38:07.591+0000] {processor.py:186} INFO - Started process (PID=17857) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:07.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:38:07.594+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:07.594+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:07.870+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:07.901+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:07.901+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:38:07.924+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:07.924+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:38:07.952+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.372 seconds
[2025-04-04T21:38:38.083+0000] {processor.py:186} INFO - Started process (PID=17923) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:38.084+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:38:38.086+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:38.085+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:38.397+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:38:38.440+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:38.439+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:38:38.466+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:38:38.466+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:38:38.495+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.425 seconds
[2025-04-04T21:39:08.563+0000] {processor.py:186} INFO - Started process (PID=17990) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:08.565+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:39:08.566+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:08.566+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:08.800+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:08.829+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:08.828+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:39:08.843+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:08.843+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:39:08.874+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.318 seconds
[2025-04-04T21:39:39.797+0000] {processor.py:186} INFO - Started process (PID=18056) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:39.798+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:39:39.800+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:39.800+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:40.040+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:39:40.070+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:40.070+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:39:40.086+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:39:40.086+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:39:40.115+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.326 seconds
[2025-04-04T21:40:10.259+0000] {processor.py:186} INFO - Started process (PID=18117) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:10.260+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:40:10.262+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:10.262+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:10.547+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:10.605+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:10.605+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:40:10.635+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:10.635+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:40:10.670+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.421 seconds
[2025-04-04T21:40:40.880+0000] {processor.py:186} INFO - Started process (PID=18183) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:40.882+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:40:40.883+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:40.883+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:41.186+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:40:41.221+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:41.221+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:40:41.240+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:40:41.240+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:40:41.278+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.407 seconds
[2025-04-04T21:41:11.777+0000] {processor.py:186} INFO - Started process (PID=18249) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:11.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:41:11.780+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:11.780+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:12.064+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:12.114+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:12.113+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:41:12.161+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:12.161+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:41:12.191+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.423 seconds
[2025-04-04T21:41:43.001+0000] {processor.py:186} INFO - Started process (PID=18315) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:43.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:41:43.004+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:43.004+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:43.293+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:41:43.322+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:43.322+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:41:43.338+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:41:43.338+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:41:43.368+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.376 seconds
[2025-04-04T21:42:13.581+0000] {processor.py:186} INFO - Started process (PID=18380) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:13.582+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:42:13.583+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:13.583+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:13.921+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:13.953+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:13.952+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:42:13.971+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:13.971+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:42:14.001+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.434 seconds
[2025-04-04T21:42:44.101+0000] {processor.py:186} INFO - Started process (PID=18447) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:44.103+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:42:44.105+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:44.104+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:44.372+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:42:44.427+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:44.426+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:42:44.447+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:42:44.446+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:42:44.481+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.388 seconds
[2025-04-04T21:43:15.473+0000] {processor.py:186} INFO - Started process (PID=18512) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:15.475+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:43:15.477+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:15.477+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:15.751+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:15.786+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:15.785+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:43:15.804+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:15.804+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:43:15.836+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.374 seconds
[2025-04-04T21:43:46.017+0000] {processor.py:186} INFO - Started process (PID=18578) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:46.018+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:43:46.020+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:46.020+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:46.308+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:43:46.355+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:46.355+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:43:46.382+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:43:46.381+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:43:46.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.405 seconds
[2025-04-04T21:44:16.515+0000] {processor.py:186} INFO - Started process (PID=18644) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:16.516+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:44:16.518+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:16.518+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:16.771+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:16.801+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:16.800+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:44:16.817+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:16.817+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:44:16.846+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.339 seconds
[2025-04-04T21:44:47.292+0000] {processor.py:186} INFO - Started process (PID=18709) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:47.293+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:44:47.295+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:47.295+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:47.564+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:44:47.600+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:47.600+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:44:47.619+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:44:47.619+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:44:47.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.367 seconds
[2025-04-04T21:45:17.881+0000] {processor.py:186} INFO - Started process (PID=18775) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:17.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:45:17.885+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:17.885+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:18.134+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:18.170+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:18.169+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:45:18.188+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:18.187+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:45:18.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.349 seconds
[2025-04-04T21:45:48.365+0000] {processor.py:186} INFO - Started process (PID=18841) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:48.366+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:45:48.368+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:48.367+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:48.662+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:45:48.696+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:48.695+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:45:48.713+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:45:48.712+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:45:48.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.387 seconds
[2025-04-04T21:46:19.629+0000] {processor.py:186} INFO - Started process (PID=18907) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:19.630+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:46:19.632+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:19.632+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:19.905+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:19.943+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:19.942+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:46:19.964+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:19.964+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:46:19.999+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.377 seconds
[2025-04-04T21:46:50.113+0000] {processor.py:186} INFO - Started process (PID=18973) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:50.115+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:46:50.116+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:50.116+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:50.410+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:46:50.440+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:50.439+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:46:50.458+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:46:50.458+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:46:50.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.386 seconds
[2025-04-04T21:47:20.710+0000] {processor.py:186} INFO - Started process (PID=19041) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:20.711+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:47:20.713+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:20.713+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:20.993+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:21.023+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:21.023+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:47:21.042+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:21.041+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:47:21.073+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.372 seconds
[2025-04-04T21:47:52.033+0000] {processor.py:186} INFO - Started process (PID=19108) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:52.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:47:52.036+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:52.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:52.389+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:47:52.425+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:52.424+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:47:52.442+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:47:52.442+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:47:52.472+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.449 seconds
[2025-04-04T21:48:22.594+0000] {processor.py:186} INFO - Started process (PID=19175) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:22.595+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:48:22.597+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:22.596+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:22.854+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:22.888+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:22.887+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:48:22.910+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:22.909+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:48:22.948+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.362 seconds
[2025-04-04T21:48:53.243+0000] {processor.py:186} INFO - Started process (PID=19239) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:53.244+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:48:53.246+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:53.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:53.530+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:48:53.575+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:53.574+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:48:53.598+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:48:53.597+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:48:53.632+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.402 seconds
[2025-04-04T21:49:23.833+0000] {processor.py:186} INFO - Started process (PID=19308) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:23.835+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:49:23.837+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:23.837+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:24.119+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:24.148+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:24.148+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:49:24.167+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:24.167+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:49:24.204+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.389 seconds
[2025-04-04T21:49:54.315+0000] {processor.py:186} INFO - Started process (PID=19372) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:54.317+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:49:54.318+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:54.318+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:54.565+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:49:54.595+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:54.594+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:49:54.614+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:49:54.614+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:49:54.650+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.343 seconds
[2025-04-04T21:50:24.811+0000] {processor.py:186} INFO - Started process (PID=19439) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:24.812+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:50:24.814+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:24.813+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:25.057+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:25.087+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:25.086+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:50:25.111+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:25.110+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:50:25.140+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.337 seconds
[2025-04-04T21:50:55.363+0000] {processor.py:186} INFO - Started process (PID=19504) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:55.364+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:50:55.366+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:55.366+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:55.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:50:55.789+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:55.788+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:50:55.826+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:50:55.826+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:50:55.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.512 seconds
[2025-04-04T21:51:25.977+0000] {processor.py:186} INFO - Started process (PID=19569) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:25.979+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:51:25.981+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:25.980+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:26.399+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:26.437+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:26.436+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:51:26.461+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:26.461+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:51:26.504+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.537 seconds
[2025-04-04T21:51:57.290+0000] {processor.py:186} INFO - Started process (PID=19636) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:57.291+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:51:57.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:57.292+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:57.546+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:51:57.574+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:57.573+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:51:57.590+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:51:57.589+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:51:57.620+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.338 seconds
[2025-04-04T21:52:27.840+0000] {processor.py:186} INFO - Started process (PID=19702) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:27.927+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:52:27.929+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:27.929+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:28.340+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:28.388+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:28.387+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:52:28.410+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:28.410+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:52:28.443+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.628 seconds
[2025-04-04T21:52:59.243+0000] {processor.py:186} INFO - Started process (PID=19770) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:59.245+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:52:59.247+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:59.246+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:59.514+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:52:59.544+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:59.544+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:52:59.562+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:52:59.561+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:52:59.626+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.402 seconds
[2025-04-04T21:53:29.779+0000] {processor.py:186} INFO - Started process (PID=19836) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:53:29.781+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:53:29.783+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:53:29.782+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:53:30.117+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:53:30.153+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:53:30.152+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:53:30.170+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:53:30.170+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:53:30.202+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.434 seconds
[2025-04-04T21:54:00.912+0000] {processor.py:186} INFO - Started process (PID=19902) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:00.913+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:54:00.915+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:00.914+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:01.520+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:01.545+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:01.544+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:54:01.560+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:01.560+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:54:01.586+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.689 seconds
[2025-04-04T21:54:31.705+0000] {processor.py:186} INFO - Started process (PID=19970) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:31.708+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:54:31.710+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:31.709+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:32.311+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:54:32.337+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:32.337+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:54:32.353+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:54:32.353+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:54:32.380+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.692 seconds
[2025-04-04T21:55:02.750+0000] {processor.py:186} INFO - Started process (PID=20036) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:02.752+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:55:02.754+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:02.753+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:03.179+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:03.203+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:03.203+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:55:03.222+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:03.222+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:55:03.247+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.504 seconds
[2025-04-04T21:55:33.580+0000] {processor.py:186} INFO - Started process (PID=20102) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:33.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:55:33.583+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:33.583+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:34.027+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:55:34.053+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:34.052+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:55:34.067+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:55:34.067+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:55:34.097+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.525 seconds
[2025-04-04T21:56:04.206+0000] {processor.py:186} INFO - Started process (PID=20171) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:04.207+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:56:04.209+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:04.209+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:04.465+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:04.705+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:04.704+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:56:04.721+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:04.721+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:56:04.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.547 seconds
[2025-04-04T21:56:35.239+0000] {processor.py:186} INFO - Started process (PID=20237) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:35.241+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:56:35.243+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:35.242+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:35.510+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:56:35.553+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:35.553+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:56:35.762+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:56:35.762+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:56:35.792+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.561 seconds
[2025-04-04T21:57:06.505+0000] {processor.py:186} INFO - Started process (PID=20303) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:06.506+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:57:06.508+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:06.507+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:06.756+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:06.785+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:06.785+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:57:07.036+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:07.036+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:57:07.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.568 seconds
[2025-04-04T21:57:37.182+0000] {processor.py:186} INFO - Started process (PID=20377) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:37.183+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:57:37.185+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:37.185+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:37.687+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:57:37.716+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:37.716+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:57:37.735+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:57:37.735+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:57:37.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.595 seconds
[2025-04-04T21:58:08.580+0000] {processor.py:186} INFO - Started process (PID=20445) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:08.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:58:08.583+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:08.583+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:09.033+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:09.058+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:09.057+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:58:09.073+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:09.073+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:58:09.102+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.530 seconds
[2025-04-04T21:58:39.491+0000] {processor.py:186} INFO - Started process (PID=20511) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:39.492+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:58:39.494+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:39.493+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:39.980+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:58:40.010+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:40.009+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:58:40.029+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:58:40.029+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:58:40.067+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.585 seconds
[2025-04-04T21:59:10.375+0000] {processor.py:186} INFO - Started process (PID=20578) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:10.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:59:10.378+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:10.377+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:10.865+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:10.897+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:10.897+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:59:10.918+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:10.918+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:59:10.949+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.583 seconds
[2025-04-04T21:59:41.016+0000] {processor.py:186} INFO - Started process (PID=20647) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:41.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T21:59:41.019+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:41.019+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:41.460+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T21:59:41.492+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:41.491+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T21:59:41.509+0000] {logging_mixin.py:190} INFO - [2025-04-04T21:59:41.508+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T21:59:41.539+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.534 seconds
[2025-04-04T22:00:11.669+0000] {processor.py:186} INFO - Started process (PID=20713) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:11.670+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:00:11.671+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:11.671+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:12.072+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:12.098+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:12.098+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:00:12.113+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:12.113+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:00:12.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.478 seconds
[2025-04-04T22:00:42.207+0000] {processor.py:186} INFO - Started process (PID=20777) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:42.209+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:00:42.211+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:42.210+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:42.833+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:00:42.921+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:42.920+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:00:42.941+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:00:42.940+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:00:42.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.770 seconds
[2025-04-04T22:01:13.777+0000] {processor.py:186} INFO - Started process (PID=20845) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:13.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:01:13.781+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:13.780+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:14.457+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:14.518+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:14.518+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:01:14.554+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:14.554+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:01:14.596+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.830 seconds
[2025-04-04T22:01:44.886+0000] {processor.py:186} INFO - Started process (PID=20911) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:44.888+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:01:44.890+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:44.890+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:45.423+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:01:45.455+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:45.455+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:01:45.474+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:01:45.473+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:01:45.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.628 seconds
[2025-04-04T22:02:16.095+0000] {processor.py:186} INFO - Started process (PID=20979) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:16.097+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:02:16.099+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:16.098+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:16.412+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:16.442+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:16.441+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:02:16.459+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:16.459+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:02:16.491+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.407 seconds
[2025-04-04T22:02:46.989+0000] {processor.py:186} INFO - Started process (PID=21044) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:46.991+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:02:46.993+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:46.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:47.349+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:02:47.417+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:47.414+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:02:47.448+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:02:47.448+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:02:47.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.528 seconds
[2025-04-04T22:03:17.703+0000] {processor.py:186} INFO - Started process (PID=21111) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:17.704+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:03:17.706+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:17.706+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:17.963+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:18.003+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:18.002+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:03:18.048+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:18.047+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:03:18.100+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.405 seconds
[2025-04-04T22:03:49.414+0000] {processor.py:186} INFO - Started process (PID=21175) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:49.416+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:03:49.419+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:49.419+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:49.717+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:03:49.749+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:49.749+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:03:49.768+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:03:49.768+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:03:49.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.401 seconds
[2025-04-04T22:04:19.975+0000] {processor.py:186} INFO - Started process (PID=21243) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:19.976+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:04:19.978+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:19.978+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:20.238+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:20.266+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:20.265+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:04:20.282+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:20.282+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:04:20.316+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.349 seconds
[2025-04-04T22:04:50.432+0000] {processor.py:186} INFO - Started process (PID=21310) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:50.433+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:04:50.435+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:50.434+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:50.674+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:04:50.716+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:50.715+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:04:50.740+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:04:50.739+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:04:50.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.350 seconds
[2025-04-04T22:05:21.608+0000] {processor.py:186} INFO - Started process (PID=21375) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:21.609+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:05:21.611+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:21.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:21.866+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:21.903+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:21.903+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:05:21.920+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:21.920+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:05:21.946+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T22:05:52.007+0000] {processor.py:186} INFO - Started process (PID=21441) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:52.009+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:05:52.011+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:52.010+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:52.279+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:05:52.316+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:52.316+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:05:52.339+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:05:52.338+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:05:52.369+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.372 seconds
[2025-04-04T22:06:10.758+0000] {processor.py:186} INFO - Started process (PID=21488) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:10.760+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:06:10.762+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:10.762+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:11.176+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:11.350+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:11.350+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:06:11.366+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:11.365+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:06:11.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.655 seconds
[2025-04-04T22:06:22.765+0000] {processor.py:186} INFO - Started process (PID=21508) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:22.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:06:22.768+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:22.767+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:23.024+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:23.038+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:23.037+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:06:23.056+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:23.056+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:06:23.089+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.334 seconds
[2025-04-04T22:06:23.288+0000] {processor.py:186} INFO - Started process (PID=21512) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:23.289+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:06:23.290+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:23.290+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:23.550+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:23.564+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:23.563+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:06:23.581+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:23.581+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:06:23.628+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.349 seconds
[2025-04-04T22:06:26.395+0000] {processor.py:186} INFO - Started process (PID=21526) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:26.396+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:06:26.399+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:26.399+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:26.668+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:26.682+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:26.681+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:06:26.698+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:26.698+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:06:26.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T22:06:57.380+0000] {processor.py:186} INFO - Started process (PID=21592) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:57.381+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:06:57.382+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:57.382+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:57.697+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:06:57.725+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:57.724+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:06:57.741+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:06:57.741+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:06:58.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.635 seconds
[2025-04-04T22:07:39.366+0000] {processor.py:186} INFO - Started process (PID=21655) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:07:39.401+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:07:39.406+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:07:39.405+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:07:48.399+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:07:48.948+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:07:48.939+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:07:49.078+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:07:49.078+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:07:49.178+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 9.837 seconds
[2025-04-04T22:08:25.731+0000] {processor.py:186} INFO - Started process (PID=21722) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:25.744+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:08:25.747+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:25.746+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:26.679+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:26.713+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:26.712+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:08:26.732+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:26.731+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:08:27.123+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.416 seconds
[2025-04-04T22:08:57.989+0000] {processor.py:186} INFO - Started process (PID=21787) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:57.991+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:08:57.993+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:57.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:58.243+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:08:58.271+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:58.270+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:08:58.289+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:08:58.289+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:08:58.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.342 seconds
[2025-04-04T22:09:29.024+0000] {processor.py:186} INFO - Started process (PID=21853) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:09:29.025+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:09:29.028+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:09:29.028+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:09:29.336+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:09:29.408+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:09:29.407+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:09:29.433+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:09:29.433+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:09:29.463+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.451 seconds
[2025-04-04T22:10:00.054+0000] {processor.py:186} INFO - Started process (PID=21920) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:00.055+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:10:00.057+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:00.056+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:00.331+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:00.367+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:00.366+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:10:00.391+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:00.390+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:10:00.414+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.371 seconds
[2025-04-04T22:10:31.061+0000] {processor.py:186} INFO - Started process (PID=21985) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:31.062+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:10:31.064+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:31.063+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:31.306+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:10:31.346+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:31.345+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:10:31.365+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:10:31.365+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:10:31.398+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T22:11:01.578+0000] {processor.py:186} INFO - Started process (PID=22053) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:01.579+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:11:01.582+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:01.581+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:01.847+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:01.876+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:01.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:11:01.894+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:01.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:11:01.923+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.354 seconds
[2025-04-04T22:11:32.070+0000] {processor.py:186} INFO - Started process (PID=22120) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:32.071+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:11:32.073+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:32.073+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:32.346+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:11:32.377+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:32.376+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:11:32.394+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:11:32.394+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:11:32.427+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.364 seconds
[2025-04-04T22:12:02.586+0000] {processor.py:186} INFO - Started process (PID=22186) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:02.587+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:12:02.589+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:02.588+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:02.877+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:02.909+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:02.909+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:12:02.929+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:02.929+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:12:02.963+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.387 seconds
[2025-04-04T22:12:33.799+0000] {processor.py:186} INFO - Started process (PID=22254) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:33.801+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:12:33.803+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:33.802+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:34.068+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:12:34.096+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:34.096+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:12:34.117+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:12:34.117+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:12:34.180+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.390 seconds
[2025-04-04T22:13:04.513+0000] {processor.py:186} INFO - Started process (PID=22321) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:04.515+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:13:04.518+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:04.517+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:04.752+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:04.791+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:04.791+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:13:04.818+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:04.818+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:13:04.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.346 seconds
[2025-04-04T22:13:35.347+0000] {processor.py:186} INFO - Started process (PID=22400) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:35.349+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:13:35.353+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:35.353+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:35.883+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:13:35.934+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:35.934+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:13:35.959+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:13:35.958+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:13:35.999+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.670 seconds
[2025-04-04T22:14:06.102+0000] {processor.py:186} INFO - Started process (PID=22466) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:06.103+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:14:06.104+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:06.104+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:06.406+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:06.436+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:06.436+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:14:06.456+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:06.456+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:14:06.481+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.392 seconds
[2025-04-04T22:14:36.582+0000] {processor.py:186} INFO - Started process (PID=22532) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:36.583+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:14:36.586+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:36.585+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:36.852+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:14:36.883+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:36.882+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:14:36.903+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:14:36.903+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:14:36.932+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.375 seconds
[2025-04-04T22:15:07.016+0000] {processor.py:186} INFO - Started process (PID=22597) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:07.018+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:15:07.020+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:07.019+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:07.262+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:07.289+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:07.289+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:15:07.307+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:07.307+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:15:07.336+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.327 seconds
[2025-04-04T22:15:37.507+0000] {processor.py:186} INFO - Started process (PID=22664) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:37.509+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:15:37.512+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:37.511+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:37.777+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:15:37.809+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:37.808+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:15:37.829+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:15:37.829+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:15:37.869+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.371 seconds
[2025-04-04T22:16:09.053+0000] {processor.py:186} INFO - Started process (PID=22730) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:09.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:16:09.056+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:09.056+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:09.293+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:09.349+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:09.348+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:16:09.377+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:09.377+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:16:09.407+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.362 seconds
[2025-04-04T22:16:39.513+0000] {processor.py:186} INFO - Started process (PID=22798) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:39.514+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:16:39.515+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:39.515+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:39.803+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:16:39.849+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:39.849+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:16:39.866+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:16:39.866+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:16:39.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.392 seconds
[2025-04-04T22:17:09.962+0000] {processor.py:186} INFO - Started process (PID=22863) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:09.963+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:17:09.965+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:09.964+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:10.267+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:10.297+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:10.296+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:17:10.316+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:10.315+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:17:10.346+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.393 seconds
[2025-04-04T22:17:40.767+0000] {processor.py:186} INFO - Started process (PID=22928) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:40.769+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:17:40.771+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:40.770+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:41.066+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:17:41.094+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:41.093+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:17:41.111+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:17:41.110+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:17:41.135+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.375 seconds
[2025-04-04T22:18:11.670+0000] {processor.py:186} INFO - Started process (PID=22993) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:11.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:18:11.674+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:11.673+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:12.054+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:12.102+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:12.102+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:18:12.131+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:12.131+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:18:12.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.503 seconds
[2025-04-04T22:18:42.222+0000] {processor.py:186} INFO - Started process (PID=23061) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:42.223+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:18:42.225+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:42.225+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:42.480+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:18:42.512+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:42.511+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:18:42.531+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:18:42.531+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:18:42.565+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.349 seconds
[2025-04-04T22:19:13.104+0000] {processor.py:186} INFO - Started process (PID=23128) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:13.106+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:19:13.108+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:13.107+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:13.372+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:13.404+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:13.403+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:19:13.421+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:13.421+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:19:13.450+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.355 seconds
[2025-04-04T22:19:43.774+0000] {processor.py:186} INFO - Started process (PID=23191) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:43.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:19:43.776+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:43.776+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:44.034+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:19:44.071+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:44.071+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:19:44.093+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:19:44.092+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:19:44.135+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.368 seconds
[2025-04-04T22:20:14.458+0000] {processor.py:186} INFO - Started process (PID=23258) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:14.460+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:20:14.461+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:14.461+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:14.702+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:14.732+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:14.732+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:20:14.754+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:14.754+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:20:14.786+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.334 seconds
[2025-04-04T22:20:45.711+0000] {processor.py:186} INFO - Started process (PID=23331) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:45.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:20:45.714+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:45.713+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:45.951+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:20:45.982+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:45.982+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:20:46.002+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:20:46.002+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:20:46.035+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.334 seconds
[2025-04-04T22:21:16.177+0000] {processor.py:186} INFO - Started process (PID=23399) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:16.178+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:21:16.179+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:16.179+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:16.439+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:16.471+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:16.471+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:21:16.490+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:16.490+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:21:16.523+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.353 seconds
[2025-04-04T22:21:46.990+0000] {processor.py:186} INFO - Started process (PID=23466) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:46.992+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:21:46.994+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:46.993+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:47.270+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:21:47.310+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:47.310+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:21:47.329+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:21:47.329+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:21:47.364+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.384 seconds
[2025-04-04T22:22:03.128+0000] {processor.py:186} INFO - Started process (PID=23493) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:03.129+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:22:03.131+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:03.131+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:03.461+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:03.490+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:03.490+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:22:03.510+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:03.509+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:22:03.547+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.429 seconds
[2025-04-04T22:22:34.170+0000] {processor.py:186} INFO - Started process (PID=23562) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:34.172+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:22:34.174+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:34.173+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:34.448+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:22:34.479+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:34.478+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:22:34.495+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:22:34.495+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:22:34.528+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.367 seconds
[2025-04-04T22:23:04.774+0000] {processor.py:186} INFO - Started process (PID=23628) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:04.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:23:04.777+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:04.777+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:05.094+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:05.132+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:05.131+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:23:05.202+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:05.201+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:23:05.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.478 seconds
[2025-04-04T22:23:35.648+0000] {processor.py:186} INFO - Started process (PID=23697) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:35.649+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:23:35.651+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:35.651+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:35.914+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:23:35.945+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:35.945+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:23:35.963+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:23:35.962+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:23:35.991+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.352 seconds
[2025-04-04T22:24:06.154+0000] {processor.py:186} INFO - Started process (PID=23765) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:06.156+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:24:06.158+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:06.157+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:06.422+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:06.461+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:06.460+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:24:06.480+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:06.479+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:24:06.513+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.368 seconds
[2025-04-04T22:24:36.619+0000] {processor.py:186} INFO - Started process (PID=23831) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:36.620+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:24:36.623+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:36.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:36.858+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:24:36.889+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:36.888+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:24:36.906+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:24:36.906+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:24:36.932+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.324 seconds
[2025-04-04T22:25:07.052+0000] {processor.py:186} INFO - Started process (PID=23898) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:07.053+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:25:07.054+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:07.054+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:07.304+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:07.336+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:07.335+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:25:07.353+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:07.352+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:25:07.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.339 seconds
[2025-04-04T22:25:37.857+0000] {processor.py:186} INFO - Started process (PID=23965) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:37.859+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:25:37.861+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:37.860+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:38.166+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:25:38.195+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:38.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:25:38.215+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:25:38.214+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:25:38.245+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.397 seconds
[2025-04-04T22:26:08.459+0000] {processor.py:186} INFO - Started process (PID=24034) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:08.461+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:26:08.463+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:08.462+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:08.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:08.756+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:08.756+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:26:08.777+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:08.777+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:26:08.806+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.355 seconds
[2025-04-04T22:26:39.704+0000] {processor.py:186} INFO - Started process (PID=24100) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:39.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:26:39.707+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:39.706+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:39.990+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:26:40.034+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:40.033+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:26:40.058+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:26:40.058+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:26:40.091+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.394 seconds
[2025-04-04T22:27:10.208+0000] {processor.py:186} INFO - Started process (PID=24166) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:10.209+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:27:10.212+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:10.211+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:10.491+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:10.525+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:10.524+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:27:10.545+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:10.545+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:27:10.585+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.390 seconds
[2025-04-04T22:27:40.788+0000] {processor.py:186} INFO - Started process (PID=24233) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:40.789+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:27:40.791+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:40.790+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:41.183+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:27:41.218+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:41.217+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:27:41.238+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:27:41.237+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:27:41.272+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.492 seconds
[2025-04-04T22:28:11.541+0000] {processor.py:186} INFO - Started process (PID=24299) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:11.543+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:28:11.544+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:11.544+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:11.866+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:11.896+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:11.895+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:28:11.914+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:11.914+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:28:12.006+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.478 seconds
[2025-04-04T22:28:42.084+0000] {processor.py:186} INFO - Started process (PID=24365) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:42.085+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:28:42.088+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:42.087+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:42.342+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:28:42.371+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:42.370+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:28:42.389+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:28:42.389+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:28:42.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.343 seconds
[2025-04-04T22:29:12.605+0000] {processor.py:186} INFO - Started process (PID=24431) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:12.606+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:29:12.608+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:12.607+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:12.849+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:12.877+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:12.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:29:12.898+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:12.897+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:29:12.930+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.333 seconds
[2025-04-04T22:29:42.990+0000] {processor.py:186} INFO - Started process (PID=24498) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:42.991+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:29:42.993+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:42.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:43.232+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:29:43.288+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:43.288+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:29:43.308+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:29:43.308+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:29:43.341+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.357 seconds
[2025-04-04T22:30:13.490+0000] {processor.py:186} INFO - Started process (PID=24565) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:13.492+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:30:13.494+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:13.493+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:13.774+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:13.809+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:13.809+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:30:13.833+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:13.833+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:30:13.871+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.387 seconds
[2025-04-04T22:30:44.321+0000] {processor.py:186} INFO - Started process (PID=24633) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:44.322+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:30:44.323+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:44.323+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:44.594+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:30:44.626+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:44.625+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:30:44.645+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:30:44.645+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:30:44.679+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.366 seconds
[2025-04-04T22:31:15.098+0000] {processor.py:186} INFO - Started process (PID=24701) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:15.099+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:31:15.102+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:15.102+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:15.637+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:15.704+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:15.703+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:31:15.755+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:15.754+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:31:15.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.726 seconds
[2025-04-04T22:31:46.242+0000] {processor.py:186} INFO - Started process (PID=24768) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:46.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:31:46.245+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:46.244+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:46.686+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:31:46.711+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:46.710+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:31:46.725+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:31:46.724+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:31:46.755+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.521 seconds
[2025-04-04T22:32:16.981+0000] {processor.py:186} INFO - Started process (PID=24835) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:16.983+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:32:16.985+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:16.984+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:17.610+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:17.642+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:17.641+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:32:17.658+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:17.658+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:32:17.687+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.716 seconds
[2025-04-04T22:32:48.393+0000] {processor.py:186} INFO - Started process (PID=24910) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:48.396+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:32:48.398+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:48.397+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:48.664+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:32:48.936+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:48.935+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:32:48.953+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:32:48.953+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:32:48.987+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.605 seconds
[2025-04-04T22:33:19.161+0000] {processor.py:186} INFO - Started process (PID=24980) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:19.163+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:33:19.164+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:19.164+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:19.424+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:19.457+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:19.456+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:33:19.668+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:19.668+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:33:19.708+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.560 seconds
[2025-04-04T22:33:50.304+0000] {processor.py:186} INFO - Started process (PID=25048) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:50.305+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:33:50.308+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:50.308+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:50.670+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:33:50.704+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:50.703+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:33:50.729+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:33:50.729+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:33:50.958+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.666 seconds
[2025-04-04T22:34:21.995+0000] {processor.py:186} INFO - Started process (PID=25116) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:21.996+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:34:21.998+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:21.998+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:22.594+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:22.655+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:22.655+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:34:22.676+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:22.676+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:34:23.108+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 1.122 seconds
[2025-04-04T22:34:53.735+0000] {processor.py:186} INFO - Started process (PID=25182) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:53.736+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:34:53.738+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:53.738+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:54.243+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:34:54.293+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:54.292+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:34:54.350+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:34:54.349+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:34:54.410+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.683 seconds
[2025-04-04T22:35:25.678+0000] {processor.py:186} INFO - Started process (PID=25250) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:25.680+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:35:25.684+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:25.683+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:26.224+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:26.252+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:26.250+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:35:26.269+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:26.269+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:35:26.300+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.633 seconds
[2025-04-04T22:35:56.930+0000] {processor.py:186} INFO - Started process (PID=25317) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:56.932+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:35:56.933+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:56.933+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:57.431+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:35:57.457+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:57.457+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:35:57.478+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:35:57.477+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:35:57.510+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.589 seconds
[2025-04-04T22:36:27.663+0000] {processor.py:186} INFO - Started process (PID=25387) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:27.664+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:36:27.666+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:27.666+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:28.095+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:28.123+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:28.122+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:36:28.138+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:28.137+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:36:28.165+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.512 seconds
[2025-04-04T22:36:58.689+0000] {processor.py:186} INFO - Started process (PID=25454) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:58.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:36:58.692+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:58.692+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:59.173+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:36:59.214+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:59.213+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:36:59.234+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:36:59.233+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:36:59.264+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.584 seconds
[2025-04-04T22:37:29.729+0000] {processor.py:186} INFO - Started process (PID=25521) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:37:29.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:37:29.738+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:37:29.737+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:37:30.373+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:37:30.403+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:37:30.402+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:37:30.420+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:37:30.420+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:37:30.448+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.770 seconds
[2025-04-04T22:38:01.014+0000] {processor.py:186} INFO - Started process (PID=25588) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:01.016+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:38:01.018+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:01.017+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:01.292+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:01.325+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:01.324+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:38:01.342+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:01.342+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:38:01.366+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.362 seconds
[2025-04-04T22:38:31.465+0000] {processor.py:186} INFO - Started process (PID=25655) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:31.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:38:31.468+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:31.468+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:31.719+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:38:31.751+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:31.750+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:38:31.767+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:38:31.767+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:38:31.793+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.338 seconds
[2025-04-04T22:39:01.950+0000] {processor.py:186} INFO - Started process (PID=25723) to work on /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:39:01.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/test_crawl_list_product.py for tasks to queue
[2025-04-04T22:39:01.953+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:39:01.953+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:39:02.210+0000] {processor.py:925} INFO - DAG(s) 'scrape_amazon_dag' retrieved from /opt/airflow/dags/test_crawl_list_product.py
[2025-04-04T22:39:02.245+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:39:02.244+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-04-04T22:39:02.262+0000] {logging_mixin.py:190} INFO - [2025-04-04T22:39:02.262+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_amazon_dag to None, run_after=None
[2025-04-04T22:39:02.296+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/test_crawl_list_product.py took 0.353 seconds
