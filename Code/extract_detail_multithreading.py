from concurrent.futures import ThreadPoolExecutor
import csv
import datetime
import threading
import mysql.connector
import logging
import math
import os
from pathlib import Path
import random
import re
import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from fake_useragent import UserAgent

ua = UserAgent()
user_agents = ua.random

def create_connection():
    # Thay ƒë·ªïi th√¥ng tin k·∫øt n·ªëi theo c∆° s·ªü d·ªØ li·ªáu c·ªßa b·∫°n
    connection = mysql.connector.connect(
        host="localhost",
        user="root",  # T√™n ng∆∞·ªùi d√πng
        password="phuocle11001203",  # M·∫≠t kh·∫©u c·ªßa b·∫°n
        database="stg_datn"  # T√™n c∆° s·ªü d·ªØ li·ªáu
    )
    return connection

# üìå T·∫°o tr√¨nh duy·ªát Selenium v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
def create_driver(user_agents):
    chrome_options = Options()

    chrome_options.add_argument("--headless")  # Ch·∫°y kh√¥ng hi·ªÉn th·ªã giao di·ªán
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f"user-agent={user_agents}")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument("--enable-unsafe-swiftshader")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--log-level=3")

    return webdriver.Chrome(options=chrome_options)

# üìå L√†m s·∫°ch d·ªØ li·ªáu vƒÉn b·∫£n
def clean_text(text):
    return re.sub(r'[\u200e]', '', text)

# üìå H√†m tr√≠ch xu·∫•t ƒë√°nh gi√° sao
def extract_review_percentages(soup):
    ratings_data = soup.find_all('a', {'class': '_cr-ratings-histogram_style_histogram-row-container__Vh7Di'})
    rating_percentages = {}

    for rating in ratings_data:
        label = rating.get('aria-label')
        if label:
            try:
                # Tr√≠ch xu·∫•t ph·∫ßn trƒÉm tr·ª±c ti·∫øp t·ª´ chu·ªói
                if "5 stars" in label:
                    rating_percentages['5-star'] = label.split(' ')[0]
                elif "4 stars" in label:
                    rating_percentages['4-star'] = label.split(' ')[0]
                elif "3 stars" in label:
                    rating_percentages['3-star'] = label.split(' ')[0]
                elif "2 stars" in label:
                    rating_percentages['2-star'] = label.split(' ')[0]
                elif "1 star" in label:
                    rating_percentages['1-star'] = label.split(' ')[0]
            except IndexError:
                # Handle any unexpected errors
                print(f"Error extracting data for label: {label}")
    
    return rating_percentages

# üìå H√†m tr√≠ch xu·∫•t m√¥ t·∫£ s·∫£n ph·∫©m
def extract_description(soup):
    # T√¨m th·∫ª ch·ª©a m√¥ t·∫£ s·∫£n ph·∫©m
    feature_bullets = soup.find('ul', class_='a-unordered-list a-vertical a-spacing-mini')

    # L·∫•y t·∫•t c·∫£ c√°c m·ª•c trong danh s√°ch <ul>
    if feature_bullets:
        items = feature_bullets.find_all('li', class_='a-spacing-mini')
        descriptions = []
        for item in items:
            description = item.get_text(strip=True)  # L·∫•y vƒÉn b·∫£n trong t·ª´ng th·∫ª <li>
            descriptions.append(description)
        
    else:
        return "Kh√¥ng t√¨m th·∫•y ph·∫ßn m√¥ t·∫£ s·∫£n ph·∫©m."
    
    return descriptions
# üìå H√†m tr√≠ch xu·∫•t link ·∫£nh
def extract_image_link(soup):
    image_tag = soup.find('img', id='landingImage')
    return image_tag['src'] if image_tag and 'src' in image_tag.attrs else 'No Image'

# üìå H√†m tr√≠ch xu·∫•t t√™n c·ª≠a h√†ng
def extract_store_name(soup):
    divsshop = soup.find('a', class_='a-link-normal', id='bylineInfo')
    return divsshop.get_text().replace("Visit the ", "") if divsshop else "None"

# üìå H√†m x·ª≠ l√Ω d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ `detailproduct_bucket`
def detailproduct_bucket(soup, divs2I,driver):
    # Tr√≠ch xu·∫•t link ·∫£nh
    description = extract_description(soup)
    rating_percentages = extract_review_percentages(soup)

    dimension = asinid = datefirstavailable = manufactures = country = sellerrank = color = modelnumber = weight = price = priceamzship = eic = shipping_days = 'None'

    divs3 = divs2I.find_all('span', class_='a-text-bold')
    for div in divs3:
        if 'Package Dimensions' in div.text.strip() or 'Dimensions' in div.text.strip():
            div4 = div.find_next_sibling('span')
            dimension = clean_text(div4.text.strip() if div4 else 'None')
        elif 'Item model number' in div.text.strip():
            div4 = div.find_next_sibling('span')
            modelnumber = clean_text(div4.text.strip() if div4 else 'None')
        elif 'Date_First_Available' in div.text.strip():
            div4 = div.find_next_sibling('span')
            datefirstavailable = clean_text(div4.text.strip() if div4 else 'None')
        elif 'Manufacturer' in div.text.strip():
            div4 = div.find_next_sibling('span')
            manufactures = clean_text(div4.text.strip() if div4 else 'None')
        elif 'ASIN' in div.text.strip():
            div4 = div.find_next_sibling('span')
            asinid = clean_text(div4.text.strip() if div4 else 'None')
        elif 'Country_of_Origin' in div.text.strip():
            div4 = div.find_next_sibling('span')
            country = clean_text(div4.text.strip() if div4 else 'None')
        elif 'Best_Sellers_Rank' in div.text.strip():
            parent_tag = div.parent
            sellerrank = clean_text(parent_tag.get_text(strip=True))
        elif 'Color' in div.text.strip():
            div4 = div.find_next_sibling('span')
            color = clean_text(div4.text.strip() if div4 else 'Not Given')

    # T√¨m v√† click v√†o n√∫t "Details" ƒë·ªÉ m·ªü b·∫£ng shipping
    try:
        details_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "span.a-declarative [role='button']"))
        )
        driver.execute_script("arguments[0].click();", details_button)
        
        # ƒê·ª£i cho b·∫£ng shipping hi·ªán ra
        shipping_table = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "a-lineitem"))
        )
        
        # Parse th√¥ng tin t·ª´ b·∫£ng m·ªõi
        table_html = shipping_table.get_attribute('outerHTML')
        table_soup = BeautifulSoup(table_html, 'html.parser')
        rows = table_soup.find_all('tr')
        
        price = priceamzship = eic = 'None'  # Kh·ªüi t·∫°o gi√° tr·ªã m·∫∑c ƒë·ªãnh

        for row in rows:
            td_key = row.find('td', class_='a-span9 a-text-left')
            td_value = row.find('td', class_='a-span2 a-text-right')
            
            if td_key and td_value:
                key_span = td_key.find('span', class_='a-size-base a-color-secondary')
                value_span = td_value.find('span', class_='a-size-base a-color-base')
                
                if key_span and value_span:
                    key_text = key_span.get_text(strip=True)
                    value_text = value_span.get_text(strip=True)
                    
                    # Tr√≠ch xu·∫•t th√¥ng tin theo t·ª´ng m·ª•c
                    if 'Price' in key_text:
                        price = value_text
                    elif 'Amazon_Global_Shipping' in key_text:
                        priceamzship = value_text
                    elif 'Estimated_Import_Charges' in key_text:
                        eic = value_text

        logging.info(f"Successfully extracted shipping details: Price={price}, Shipping={priceamzship}, Import={eic}")
    except Exception as e:
        logging.warning(f"Could not extract shipping details: {str(e)}")
        price = priceamzship = eic = 'None'


    shop_name = extract_store_name(soup)
    
    return {
        "Dimension": dimension,
        "ASIN": asinid,
        "Date_First_Available": datefirstavailable,
        "Manufacturer": manufactures,
        "Store": shop_name,
        "Country_of_Origin": country,
        "Best_Sellers_Rank": sellerrank,
        "Color": color,
        "Item_Model_Number": modelnumber,
        "Item_Weight": weight,
        "Price": price,
        "Amazon_Global_Shipping": priceamzship,
        "Estimated_Import_Charges": eic,
        "Description" : description,
        "5-star": rating_percentages.get('5-star', '0%'),
        "4-star": rating_percentages.get('4-star', '0%'),
        "3-star": rating_percentages.get('3-star', '0%'),
        "2-star": rating_percentages.get('2-star', '0%'),
        "1-star": rating_percentages.get('1-star', '0%')

    }

def detailproduct_table(soup, divs2,driver):
        # Tr√≠ch xu·∫•t link ·∫£nh
    shop_name = extract_store_name(soup)
    description = extract_description(soup)
    rating_percentages = extract_review_percentages(soup)
    dimension = asinid = datefirstavailable = manufactures = country = sellerrank = color = modelnumber = weight = price = priceamzship = eic = shipping_days = 'None'

    divs3 = divs2.find_all('th', class_='a-color-secondary a-size-base prodDetSectionEntry')
    for div in divs3:
        if 'Manufacturer' in div.text.strip():
            divs7 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            manufactures = clean_text(divs7.text.strip() if divs7 else 'None')
        if 'Dimensions' in div.text.strip():
            divs4 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            dimension = clean_text(divs4.text.strip() if divs4 else 'None')
        if 'ASIN' in div.text.strip():
            divs5 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            asinid = clean_text(divs5.text.strip() if divs5 else 'None')
        if 'Date_First_Available' in div.text.strip():
            divs6 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            datefirstavailable = clean_text(divs6.text.strip() if divs6 else 'None')
        if 'Country_of_Origin' in div.text.strip():
            divs8 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            country = clean_text(divs8.text.strip() if divs8 else 'None')
        if 'Best_Sellers_Rank' in div.text.strip():
            divs9 = div.find_next_sibling('td')
            sellerrank = clean_text(divs9.text.strip() if divs9 else 'None')
        if 'Color' in div.text.strip():
            divs10 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            color = clean_text(divs10.text.strip() if divs10 else 'Not Given')
        if 'Item model number' in div.text.strip():
            divs11 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            modelnumber = clean_text(divs11.text.strip() if divs11 else 'Not Given')
        if 'Item_Weight' in div.text.strip():
            divs12 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')
            weight = clean_text(divs12.text.strip() if divs12 else 'Not Given')

        # Th√¥ng tin gi√° shipping
        try:
            # T√¨m v√† click v√†o n√∫t Details
            details_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "span.a-declarative [role='button']"))
            )
            driver.execute_script("arguments[0].click();", details_button)
            
            # ƒê·ª£i cho b·∫£ng shipping hi·ªán ra
            shipping_table = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "a-lineitem"))
            )
            
            # Parse th√¥ng tin t·ª´ b·∫£ng m·ªõi
            table_html = shipping_table.get_attribute('outerHTML')
            table_soup = BeautifulSoup(table_html, 'html.parser')
            rows = table_soup.find_all('tr')
            
            for row in rows:
                td_key = row.find('td', class_='a-span9 a-text-left')
                td_value = row.find('td', class_='a-span2 a-text-right')
                
                if td_key and td_value:
                    key_span = td_key.find('span', class_='a-size-base a-color-secondary')
                    value_span = td_value.find('span', class_='a-size-base a-color-base')
                    
                    if key_span and value_span:
                        key_text = key_span.get_text(strip=True)
                        value_text = value_span.get_text(strip=True)
                        
                        if 'Price' in key_text:
                            price = value_text
                        elif 'Amazon_Global_Shipping' in key_text:
                            priceamzship = value_text
                        elif 'Estimated_Import_Charges' in key_text:
                            eic = value_text
            
            logging.info(f"Successfully extracted shipping details: Price={price}, Shipping={priceamzship}, Import={eic}")
        
        except Exception as e:
            logging.warning(f"Could not extract shipping details: {str(e)}")
            price = priceamzship = eic = 'None'
            


    
    return {
        "Dimension": dimension,
        "ASIN": asinid,
        "Date_First_Available": datefirstavailable,
        "Manufacturer": manufactures,
        "Store": shop_name,
        "Country_of_Origin": country,
        "Best_Sellers_Rank": sellerrank,
        "Color": color,
        "Item_Model_Number": modelnumber,
        "Item_Weight": weight,
        "Price": price,
        "Amazon_Global_Shipping": priceamzship,
        "Estimated_Import_Charges": eic,
        "Description" : description,
        "5-star": rating_percentages.get('5-star', '0%'),
        "4-star": rating_percentages.get('4-star', '0%'),
        "3-star": rating_percentages.get('3-star', '0%'),
        "2-star": rating_percentages.get('2-star', '0%'),
        "1-star": rating_percentages.get('1-star', '0%')

    }

# üìå H√†m l∆∞u k·∫øt qu·∫£ v√†o file CSV ƒë·∫ßu ra
# üìå H√†m l∆∞u k·∫øt qu·∫£ v√†o file CSV v·ªõi t√™n do b·∫°n t·ª± ch·ªâ ƒë·ªãnh
def save_result_to_csv(data, output_file="output_data.csv"):
    fieldnames = ["Dimension", "ASIN", "Date_First_Available", "Manufacturer", "Store", "Country_of_Origin", 
                  "Best_Sellers_Rank", "Color", "Item_Model_Number", "Item_Weight", "Price", 
                  "Amazon_Global_Shipping", "Estimated_Import_Charges", "Description", "5-star", "4-star", 
                  "3-star", "2-star", "1-star"]

    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    file_exists = output_path.exists()

    with open(output_path, 'a', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

        if not file_exists:
            csv_writer.writeheader()

        csv_writer.writerow(data)
        csv_file.flush()

# def save_result_to_mysql(data, lock=None):
#     # ƒê·∫£m b·∫£o r·∫±ng ch·ªâ m·ªôt lu·ªìng c√≥ th·ªÉ ghi v√†o MySQL t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
#     if lock:
#         lock.acquire()
    
#     try:
#         connection = create_connection()
#         cursor = connection.cursor()

#         # C√¢u l·ªánh SQL ƒë·ªÉ ch√®n d·ªØ li·ªáu v√†o b·∫£ng
#         insert_query = """
#         INSERT INTO detail_product_data (Dimension, ASIN, Date_First_Available, Manufacturer, Store, Country_of_Origin, 
#         Best_Sellers_Rank, Color, Item_Model_Number, Item_Weight, Price, AmazonGlobal_Shipping, Estimated_Import_Charges, 
#         Description, `5-star`, `4-star`, `3-star`, `2-star`, `1-star`)
#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#         """

#         values = (
#             data["Dimension"], 
#             data["ASIN"], 
#             data["Date_First_Available"], 
#             data["Manufacturer"], 
#             data["Store"], 
#             data["Country_of_Origin"], 
#             data["Best_Sellers_Rank"], 
#             data["Color"], 
#             data["Item_Model_Number"], 
#             data["Item_Weight"], 
#             data["Price"], 
#             data["Amazon_Global_Shipping"], 
#             data["Estimated_Import_Charges"], 
#             data["Description"],
#             data["5-star"], 
#             data["4-star"], 
#             data["3-star"], 
#             data["2-star"], 
#             data["1-star"]
#         )

#         # Th·ª±c thi c√¢u l·ªánh ch√®n d·ªØ li·ªáu
#         cursor.execute(insert_query, values)

#         # Commit ƒë·ªÉ l∆∞u thay ƒë·ªïi v√†o c∆° s·ªü d·ªØ li·ªáu
#         connection.commit()

#         # ƒê√≥ng k·∫øt n·ªëi
#         cursor.close()
#         connection.close()

#         logging.info(f"Data inserted into MySQL: {data['ASIN']}")
#     finally:
#         if lock:
#             lock.release()  # Th·∫£ lock ƒë·ªÉ cho ph√©p lu·ªìng kh√°c ghi v√†o MySQL


# H√†m crawl th√¥ng tin s·∫£n ph·∫©m
def crawl_product(url, user_agents):
    driver = create_driver(user_agents)
    data = {}
    driver.get(url)
    # Ki·ªÉm tra User-Agent t·ª´ tr√¨nh duy·ªát th·∫≠t
    user_agent_check = driver.execute_script("return navigator.userAgent;")
    print(f"üîç Thread {os.getpid()} is using User-Agent: {user_agent_check}")

    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        divs2 = soup.find('div', id='prodDetails')
        divs2I = soup.find('div', id='detailBulletsWrapper_feature_div')

        if divs2:
            data = detailproduct_table(soup, divs2, driver)
        elif divs2I:
            data = detailproduct_bucket(soup, divs2I, driver)

        print(f"‚úÖ Crawled: {url}")
        print(data)
        # Sau khi thu th·∫≠p d·ªØ li·ªáu, ngay l·∫≠p t·ª©c l∆∞u v√†o MySQL
        save_result_to_csv(data)  # L∆∞u d·ªØ li·ªáu v√†o MySQL
    finally:
        driver.quit()

    return data

# H√†m th√™m ƒë·ªô tr·ªÖ ng·∫´u nhi√™n tr∆∞·ªõc khi crawl
def crawl_product_with_delay(url, user_agents):
    delay = random.uniform(1, 5)  # ƒê·ªô tr·ªÖ ng·∫´u nhi√™n t·ª´ 1 ƒë·∫øn 5 gi√¢y
    print(f"‚è≥ Process {os.getpid()} waiting {delay:.2f}s before crawling {url}")
    time.sleep(delay)  # Ch·ªù tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu crawl
    return crawl_product(url, user_agents)  # G·ªçi h√†m crawl ch√≠nh

# üìå H√†m x·ª≠ l√Ω crawling v·ªõi m·ªôt file CSV duy nh·∫•t
def crawl_file(file_path, user_agents, start_idx, num_urls_per_file=100, lock=None):
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
    df = pd.read_csv(file_path)
    urls = df['Link'].tolist()

    # L·∫•y c√°c URL c·∫ßn crawl
    urls_to_crawl = urls[start_idx:start_idx+num_urls_per_file]
    results = []

    # D√πng lock ƒë·ªÉ ƒë·ªìng b·ªô c√°c lu·ªìng
    if lock:
        lock.acquire()  # ƒê·∫£m b·∫£o ch·ªâ m·ªôt lu·ªìng c√≥ th·ªÉ x·ª≠ l√Ω file t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
        try:
            print(f"üìÇ ƒêang crawl file: {file_path}")  # In ra t√™n file ƒëang ƒë∆∞·ª£c crawl

            for url in urls_to_crawl:
                result = crawl_product_with_delay(url, user_agents)
                results.append(result)

                # L∆∞u k·∫øt qu·∫£ v√†o file CSV ƒë·∫ßu ra
                save_result_to_csv(result, file_path.replace("part1", "part2"))

            print(f"‚úÖ Finished processing file: {file_path} from index {start_idx}")

        finally:
            lock.release()  # Th·∫£ lock ƒë·ªÉ lu·ªìng kh√°c c√≥ th·ªÉ x·ª≠ l√Ω

# üìå H√†m x·ª≠ l√Ω crawling v·ªõi nhi·ªÅu lu·ªìng song song
def multi_crawl(file_path, user_agents, num_threads, num_urls_per_file=100):
    lock = threading.Lock()  # Kh·ªüi t·∫°o lock ƒë·ªÉ ƒë·ªìng b·ªô c√°c lu·ªìng

    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y nhi·ªÅu lu·ªìng song song
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = []

        # Chia nh·ªè URL trong file th√†nh t·ª´ng ph·∫ßn (100 URL m·ªói l·∫ßn)
        total_urls = len(pd.read_csv(file_path)['Link'].tolist())
        for start_idx in range(0, total_urls, num_urls_per_file):
            futures.append(executor.submit(crawl_file, file_path, user_agents[start_idx % len(user_agents)], start_idx, num_urls_per_file, lock))

        # Ch·ªù ho√†n th√†nh t·∫•t c·∫£ c√°c t√°c v·ª•
        for future in futures:
            future.result()

    print("üèÅ All crawling tasks completed!")

# üìå Ch·∫°y ch∆∞∆°ng tr√¨nh ch√≠nh
if __name__ == "__main__":
    #input_directory = "D:\\LNTP ·ªü HUST\\H·ªçc t·∫≠p\\NƒÉm 4\\20242\\ƒêATN\\Code\\part1_20250409"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c file CSV
    input_file = "D:\\LNTP ·ªü HUST\\H·ªçc t·∫≠p\\NƒÉm 4\\20242\\ƒêATN\\Code\\part1_20250409\\part1_data_Automotive_Oils & Fluids_20250409.csv"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV duy nh·∫•t

    num_threads = 3  # S·ªë l∆∞·ª£ng lu·ªìng song song
    print(f"üìå ƒêang crawl t·ª´ c√°c file CSV trong th∆∞ m·ª•c {input_file} v·ªõi {num_threads} lu·ªìng song song...")

    # B·∫Øt ƒë·∫ßu crawl    
    multi_crawl(input_file, user_agents ,num_threads)


    print("‚úÖ Ho√†n th√†nh crawling!")