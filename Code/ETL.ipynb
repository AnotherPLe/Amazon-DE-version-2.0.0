{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0e7e5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2f9e2",
   "metadata": {},
   "source": [
    "#### Quy trình tiền xử lý\n",
    "1. Tiền xử lý file danh sách sản phẩm\n",
    "\n",
    "    a. Merge tất cả danh sách folder\n",
    "    \n",
    "    b. Cân chỉnh cột discount: giá trị ban đầu đang lẫn cả $ và %\n",
    "    \n",
    "    thêm cột Coupon discount type:\n",
    "     \n",
    "    + nếu giá trị trong ô Coupon discount là một %, thêm vào Coupon discount type là percentage voucher discount discount\n",
    "    + còn nếu giá trị trong ô Coupon discount đang là một giá trị tiền cụ thể, thêm vào Coupon discount_type là value discount voucher\n",
    "    + còn nếu trong ô Coupon discount không có giá trị, ghi giá trị 0, thêm vào Coupon discount_type là No discount voucher\n",
    "\n",
    "    sau đó xóa bỏ ký tự % hoặc $ trong ô Coupon discount\n",
    "    \n",
    "    c. tách asin từ URL\n",
    "\n",
    "    d. Đồng nhất ngày giao, tính toán cụ thể số ngày\n",
    "\n",
    "    + nếu ngày trong khoảng thì chia trung bình \n",
    "\n",
    "2. Tiền xử lý file chi tiết từng sản phẩm\n",
    "\n",
    "    a. Chỉnh sửa giá trị cột Dimension: \n",
    "\n",
    "    + tính giá trị thể tích dựa vào value trong dimension và thêm cột thể tích (chuyển từ inches về cm3), loại bỏ ký tự thừa\n",
    "    + chuyển giá trị sau dấu; về item weight, đồng nhất đơn vị trong item weight (là ounces), loại bỏ ký tự thừa\n",
    "    + Thêm cột:\n",
    "        - khoảng phân loại cho giá trị item weight\n",
    "        - khoảng giá tiền\n",
    "        - Thêm cột châu lục\n",
    "    + Xóa cột không dùng: Item Model Number\n",
    "    + Fill-up cột color: có màu cụ thể thì sẽ giữ nguyên, còn nếu đang none thì sẽ fill là: Multicolor\n",
    "    + Định dạng lại các cột \n",
    "        - giá tiền\n",
    "        - thời gian\n",
    "\n",
    "3. Merger file chi tiết sản phẩm và file danh sách sản phẩm theo ASIN rồi remove duplicate\n",
    "\n",
    "4. Đẩy vào datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21686008",
   "metadata": {},
   "source": [
    "# 1. Xử lý file danh sách sản phẩm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa03a3",
   "metadata": {},
   "source": [
    "# 2. Xử lý file chi tiết sản phẩm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "62d63191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This MC\\AppData\\Local\\Temp\\ipykernel_10400\\3794253268.py:1: ParserWarning: Skipping line 498: expected 19 fields, saw 38\n",
      "Skipping line 9681: expected 19 fields, saw 31\n",
      "Skipping line 18531: expected 19 fields, saw 23\n",
      "Skipping line 21728: expected 19 fields, saw 25\n",
      "Skipping line 23569: expected 19 fields, saw 25\n",
      "Skipping line 26964: expected 19 fields, saw 21\n",
      "\n",
      "  df_prt2 = pd.read_csv('D:\\\\LNTP ở HUST\\\\Học tập\\\\Năm 4\\\\20242\\\\ĐATN\\\\Data\\\\output_data_3 copy.csv',on_bad_lines='warn')\n",
      "C:\\Users\\This MC\\AppData\\Local\\Temp\\ipykernel_10400\\3794253268.py:1: ParserWarning: Skipping line 41458: expected 19 fields, saw 23\n",
      "Skipping line 42222: expected 19 fields, saw 51\n",
      "Skipping line 54654: expected 19 fields, saw 24\n",
      "\n",
      "  df_prt2 = pd.read_csv('D:\\\\LNTP ở HUST\\\\Học tập\\\\Năm 4\\\\20242\\\\ĐATN\\\\Data\\\\output_data_3 copy.csv',on_bad_lines='warn')\n",
      "C:\\Users\\This MC\\AppData\\Local\\Temp\\ipykernel_10400\\3794253268.py:1: ParserWarning: Skipping line 68066: expected 19 fields, saw 42\n",
      "Skipping line 69148: expected 19 fields, saw 22\n",
      "Skipping line 72467: expected 19 fields, saw 25\n",
      "Skipping line 83638: expected 19 fields, saw 56\n",
      "Skipping line 89105: expected 19 fields, saw 31\n",
      "\n",
      "  df_prt2 = pd.read_csv('D:\\\\LNTP ở HUST\\\\Học tập\\\\Năm 4\\\\20242\\\\ĐATN\\\\Data\\\\output_data_3 copy.csv',on_bad_lines='warn')\n"
     ]
    }
   ],
   "source": [
    "df_prt2 = pd.read_csv('D:\\\\LNTP ở HUST\\\\Học tập\\\\Năm 4\\\\20242\\\\ĐATN\\\\Data\\\\output_data_3 copy.csv',on_bad_lines='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ccff758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This MC\\AppData\\Local\\Temp\\ipykernel_10400\\1391159824.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_prt2 = df_prt2.applymap(lambda x: x.replace('\\n', ' ').replace('\\r', ' ') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ dấu xuống dòng trong cột mô tả (nếu cột mô tả là 'Description')\n",
    "df_prt2['Description'] = df_prt2['Description'].str.replace('\\n', ' ').str.replace('\\r', ' ')\n",
    "\n",
    "# Hoặc áp dụng cho toàn bộ dataframe để loại bỏ tất cả các dấu ngắt dòng trong mọi cột\n",
    "df_prt2 = df_prt2.applymap(lambda x: x.replace('\\n', ' ').replace('\\r', ' ') if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "71d10e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part1 = pd.read_csv('D:\\\\LNTP ở HUST\\\\Học tập\\\\Năm 4\\\\20242\\\\ĐATN\\\\Data_overview\\\\merged_data_part1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5abe4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_part1, df_prt2, on='ASIN', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8112387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số null trong cột 'Department' : 0\n",
      "Số null trong cột 'Sub Department' : 0\n",
      "Số null trong cột 'Product Name' : 0\n",
      "Số null trong cột 'Price_x' : 533\n",
      "Số null trong cột 'Rating' : 33671\n",
      "Số null trong cột 'Review' : 33671\n",
      "Số null trong cột 'Link' : 0\n",
      "Số null trong cột 'Image URL' : 0\n",
      "Số null trong cột 'Day Delivered' : 12283\n",
      "Số null trong cột 'Date Added' : 0\n",
      "Số null trong cột 'ASIN' : 17532\n",
      "Số null trong cột 'Dimension' : 17585\n",
      "Số null trong cột 'Date_First_Available' : 36861\n",
      "Số null trong cột 'Manufacturer' : 17220\n",
      "Số null trong cột 'Store' : 15\n",
      "Số null trong cột 'Country_of_Origin' : 55703\n",
      "Số null trong cột 'Best_Sellers_Rank' : 24526\n",
      "Số null trong cột 'Color' : 69101\n",
      "Số null trong cột 'Item_Model_Number' : 25408\n",
      "Số null trong cột 'Item_Weight' : 33818\n",
      "Số null trong cột 'Price_y' : 19172\n",
      "Số null trong cột 'Amazon_Global_Shipping' : 19172\n",
      "Số null trong cột 'Estimated_Import_Charges' : 19172\n",
      "Số null trong cột 'Description' : 6\n",
      "Số null trong cột '5-star' : 6\n",
      "Số null trong cột '4-star' : 6\n",
      "Số null trong cột '3-star' : 6\n",
      "Số null trong cột '2-star' : 6\n",
      "Số null trong cột '1-star' : 6\n"
     ]
    }
   ],
   "source": [
    "for col in df_final.columns:\n",
    "    null_count = df_final[col].isnull().sum()\n",
    "    print(f\"Số null trong cột '{col}' : {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f450e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates('ASIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "85d4a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Hàm trích xuất Main Ranking Value từ chuỗi\n",
    "def extract_main_ranking_value(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Loại bỏ đoạn trong dấu ngoặc tròn, ví dụ: (See Top 100...)\n",
    "    cleaned = re.sub(r'\\(.*?\\)', '', str(text))\n",
    "    \n",
    "    # Tìm giá trị đầu tiên sau dấu \"#\" và trước \"in\"\n",
    "    match = re.search(r'#([\\d,]+)\\s+in', cleaned)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))  # chuyển sang số nguyên\n",
    "    return None\n",
    "\n",
    "# Áp dụng vào DataFrame\n",
    "df_final['Main Ranking Value'] = df_final['Best_Sellers_Rank'].apply(extract_main_ranking_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "274dc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Thay thế chuỗi 'NaN' bằng giá trị NaN thực sự\n",
    "df_final['Dimension'] = df_final['Dimension'].replace('NaN', np.nan)\n",
    "\n",
    "# Hàm trích xuất trọng lượng từ chuỗi Dimension và chuyển đổi sang ounces\n",
    "def extract_weight(dim_str):\n",
    "    if pd.isna(dim_str) or \";\" not in dim_str:\n",
    "        return np.nan\n",
    "    \n",
    "    # Trích xuất phần văn bản sau dấu chấm phẩy chứa thông tin trọng lượng\n",
    "    weight_part = dim_str.split(\";\")[1].strip()\n",
    "    \n",
    "    # Trích xuất giá trị số\n",
    "    weight_value = re.search(r'(\\d+\\.?\\d*)', weight_part)\n",
    "    if weight_value:\n",
    "        weight_value = float(weight_value.group(1))\n",
    "        \n",
    "        # Chuyển đổi sang ounces nếu cần\n",
    "        if \"pound\" in weight_part.lower() or \"lb\" in weight_part.lower():\n",
    "            weight_ounces = weight_value * 16  # 1 pound = 16 ounces\n",
    "        else:  # Đã là ounces\n",
    "            weight_ounces = weight_value\n",
    "            \n",
    "        return weight_ounces\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "# Hàm làm sạch chuỗi Dimension\n",
    "def clean_dimension(dim_str):\n",
    "    if pd.isna(dim_str):\n",
    "        return np.nan\n",
    "    \n",
    "    # Trích xuất phần Dimension (trước dấu chấm phẩy nếu có)\n",
    "    if \";\" in dim_str:\n",
    "        dim_str = dim_str.split(\";\")[0]\n",
    "    \n",
    "    # Trích xuất các số từ chuỗi\n",
    "    numbers = re.findall(r'(\\d+\\.?\\d*)', dim_str)\n",
    "    \n",
    "    # Kiểm tra xem có đủ kích thước để làm việc không\n",
    "    if len(numbers) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Định dạng dựa trên số lượng kích thước có sẵn\n",
    "    if len(numbers) >= 3:\n",
    "        return f\"{numbers[0]} x {numbers[1]} x {numbers[2]} inches\"\n",
    "    elif len(numbers) == 2:\n",
    "        return f\"{numbers[0]} x {numbers[1]} inches\"\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Hàm tính thể tích\n",
    "def calculate_volume(dim_str):\n",
    "    if pd.isna(dim_str):\n",
    "        return np.nan\n",
    "    \n",
    "    # Trích xuất các số từ chuỗi\n",
    "    numbers = re.findall(r'(\\d+\\.?\\d*)', dim_str)\n",
    "    \n",
    "    # Chuyển đổi sang số thực\n",
    "    numbers = [float(num) for num in numbers]\n",
    "    \n",
    "    # Tính thể tích dựa trên kích thước có sẵn\n",
    "    if len(numbers) >= 3:\n",
    "        # Nếu có ít nhất 3 số, giả định chúng là chiều dài, rộng, cao\n",
    "        volume_inches = numbers[0] * numbers[1] * numbers[2]\n",
    "    elif len(numbers) == 2:\n",
    "        # Nếu chỉ có 2 kích thước, giả định đó là vật phẳng với chiều cao không đáng kể (0.1 inch)\n",
    "        volume_inches = numbers[0] * numbers[1] * 0.1\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    # Chuyển đổi từ inch³ sang cm³ (1 inch³ = 16.387064 cm³)\n",
    "    volume_cm3 = volume_inches * 16.387064\n",
    "    \n",
    "    # Làm tròn đến 2 chữ số thập phân\n",
    "    return round(volume_cm3, 2)\n",
    "\n",
    "# Trích xuất thông tin trọng lượng trước và chuẩn hóa sang ounces\n",
    "df_final['Item_Weight'] = df_final['Dimension'].apply(extract_weight)\n",
    "\n",
    "# Làm sạch cột Dimension và tính thể tích\n",
    "df_final['clean_dimension'] = df_final['Dimension'].apply(clean_dimension)\n",
    "df_final['volume_cm3'] = df_final['Dimension'].apply(calculate_volume)\n",
    "\n",
    "# Thay thế cột Dimension gốc bằng phiên bản đã làm sạch\n",
    "df_final['Dimension'] = df_final['clean_dimension']\n",
    "df_final = df_final.drop(columns=['clean_dimension'])\n",
    "\n",
    "# Làm tròn cột Item_Weight đến 2 chữ số thập phân\n",
    "df_final['Item_Weight'] = df_final['Item_Weight'].apply(lambda x: round(x, 2) if not pd.isna(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "df1cd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_delivery_days(row):\n",
    "    text = str(row['Day Delivered']).strip()\n",
    "    \n",
    "    # Trường hợp chỉ có số nguyên (giữ nguyên)\n",
    "    if re.fullmatch(r'\\d+', text):\n",
    "        return int(text)\n",
    "\n",
    "    # Trường hợp \"NN ngày (ngày giao ...)\"\n",
    "    match_ngay = re.match(r'(\\d+)\\s+ngày', text)\n",
    "    if match_ngay:\n",
    "        return int(match_ngay.group(1))\n",
    "    \n",
    "    # Trường hợp \"ngày giao chính xác: Apr 25\" → tính ngày thực\n",
    "    match_chinhxac = re.search(r'ngày giao chính xác: ([A-Za-z]+ \\d{1,2})', text)\n",
    "    if match_chinhxac:\n",
    "        try:\n",
    "            exact_date = pd.to_datetime(match_chinhxac.group(1) + ' 2025', format='%b %d %Y')\n",
    "            if pd.notnull(row['Date Add']):\n",
    "                return (exact_date - row['Date Add']).days\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    return None  # fallback nếu không khớp gì\n",
    "\n",
    "# Áp dụng:\n",
    "df_final['Day Delivered'] =df_final.apply(extract_delivery_days, axis=1)\n",
    "df_final['Day Delivered'] = pd.to_numeric(df_final['Day Delivered'], errors='coerce').fillna(0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e4f4cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa cột\n",
    "df_final['Price_y'] = df_final['Price_y'].str.replace('$', '')\n",
    "df_final['Price_x'] = df_final['Price_x'].str.replace('$', '')\n",
    "df_final['Amazon_Global_Shipping'] = df_final['Amazon_Global_Shipping'].str.replace('$', '')\n",
    "df_final['Estimated_Import_Charges'] = df_final['Estimated_Import_Charges'].str.replace('$', '')\n",
    "df_final['5-star'] = df_final['5-star'].str.replace('%','')\n",
    "df_final['4-star'] = df_final['4-star'].str.replace('%','')\n",
    "df_final['3-star'] = df_final['3-star'].str.replace('%','')\n",
    "df_final['2-star'] = df_final['2-star'].str.replace('%','')\n",
    "df_final['1-star'] = df_final['1-star'].str.replace('%','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c1c78279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chỉnh sửa tên một số nước\n",
    "df_final['Country_of_Origin'] = df_final['Country_of_Origin'].str.replace('Korea, Republic of', 'Korea')\n",
    "df_final['Country_of_Origin'] = df_final['Country_of_Origin'].str.replace('USA', 'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ed254070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Store'] = df_final['Store'].replace({'Brand: ': '', 'Store': ''}, regex=True)\n",
    "df_final['Store'] = df_final['Store'].str.strip()              # bỏ khoảng trắng đầu/cuối\n",
    "df_final['Store'] = df_final['Store'].str.replace(r'\\s+', ' ', regex=True)  # thay nhiều khoảng trắng bằng 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5ea86886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chỉnh sửa định dạng\n",
    "df_final['Review'] = pd.to_numeric(df_final['Review'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ab74759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Từ điển các quốc gia và châu lục\n",
    "continent_dict = {\n",
    "    'Afghanistan': 'Asia',\n",
    "    'Albania': 'Europe',\n",
    "    'Algeria': 'Africa',\n",
    "    'Andorra': 'Europe',\n",
    "    'Angola': 'Africa',\n",
    "    'Antigua and Barbuda': 'North America',\n",
    "    'Argentina': 'South America',\n",
    "    'Armenia': 'Asia',\n",
    "    'Australia': 'Oceania',\n",
    "    'Austria': 'Europe',\n",
    "    'Azerbaijan': 'Asia',\n",
    "    'American Samoa': 'Oceania',\n",
    "    'Bahamas': 'North America',\n",
    "    'Bahrain': 'Asia',\n",
    "    'Bangladesh': 'Asia',\n",
    "    'Barbados': 'North America',\n",
    "    'Belarus': 'Europe',\n",
    "    'Belgium': 'Europe',\n",
    "    'Belize': 'North America',\n",
    "    'Benin': 'Africa',\n",
    "    'Bhutan': 'Asia',\n",
    "    'Bolivia': 'South America',\n",
    "    'Bosnia and Herzegovina': 'Europe',\n",
    "    'Botswana': 'Africa',\n",
    "    'Brazil': 'South America',\n",
    "    'Brunei': 'Asia',\n",
    "    'Bulgaria': 'Europe',\n",
    "    'Burkina Faso': 'Africa',\n",
    "    'Burundi': 'Africa',\n",
    "    'Cabo Verde': 'Africa',\n",
    "    'Cambodia': 'Asia',\n",
    "    'Cameroon': 'Africa',\n",
    "    'Canada': 'North America',\n",
    "    'Central African Republic': 'Africa',\n",
    "    'Chad': 'Africa',\n",
    "    'Chile': 'South America',\n",
    "    'China': 'Asia',\n",
    "    'Colombia': 'South America',\n",
    "    'Comoros': 'Africa',\n",
    "    'Congo, Democratic Republic of the': 'Africa',\n",
    "    'Congo, Republic of the': 'Africa',\n",
    "    'Costa Rica': 'North America',\n",
    "    'Croatia': 'Europe',\n",
    "    'Cuba': 'North America',\n",
    "    'Cyprus': 'Asia',\n",
    "    'Czech Republic': 'Europe',\n",
    "    'Denmark': 'Europe',\n",
    "    'Djibouti': 'Africa',\n",
    "    'Dominica': 'North America',\n",
    "    'Dominican Republic': 'North America',\n",
    "    'Ecuador': 'South America',\n",
    "    'Egypt': 'Africa',\n",
    "    'El Salvador': 'North America',\n",
    "    'Equatorial Guinea': 'Africa',\n",
    "    'Eritrea': 'Africa',\n",
    "    'Estonia': 'Europe',\n",
    "    'Eswatini': 'Africa',\n",
    "    'Ethiopia': 'Africa',\n",
    "    'Fiji': 'Oceania',\n",
    "    'Finland': 'Europe',\n",
    "    'France': 'Europe',\n",
    "    'Gabon': 'Africa',\n",
    "    'Gambia': 'Africa',\n",
    "    'Georgia': 'Asia',\n",
    "    'Germany': 'Europe',\n",
    "    'Ghana': 'Africa',\n",
    "    'Greece': 'Europe',\n",
    "    'Grenada': 'North America',\n",
    "    'Guatemala': 'North America',\n",
    "    'Guinea': 'Africa',\n",
    "    'Guinea-Bissau': 'Africa',\n",
    "    'Guyana': 'South America',\n",
    "    'Haiti': 'North America',\n",
    "    'Honduras': 'North America',\n",
    "    'Hungary': 'Europe',\n",
    "    'Hong Kong': 'Asia',\n",
    "    'Iceland': 'Europe',\n",
    "    'India': 'Asia',\n",
    "    'Indonesia': 'Asia',\n",
    "    'Iran': 'Asia',\n",
    "    'Iraq': 'Asia',\n",
    "    'Ireland': 'Europe',\n",
    "    'Israel': 'Asia',\n",
    "    'Italy': 'Europe',\n",
    "    'Jamaica': 'North America',\n",
    "    'Japan': 'Asia',\n",
    "    'Jordan': 'Asia',\n",
    "    'Kazakhstan': 'Asia',\n",
    "    'Kenya': 'Africa',\n",
    "    'Kiribati': 'Oceania',\n",
    "    'Korea, North': 'Asia',\n",
    "    'Korea, South': 'Asia',\n",
    "    'Korea':'Asia',\n",
    "    'Kosovo': 'Europe',\n",
    "    'Kuwait': 'Asia',\n",
    "    'Kyrgyzstan': 'Asia',\n",
    "    'Laos': 'Asia',\n",
    "    'Latvia': 'Europe',\n",
    "    'Lebanon': 'Asia',\n",
    "    'Lesotho': 'Africa',\n",
    "    'Liberia': 'Africa',\n",
    "    'Libya': 'Africa',\n",
    "    'Liechtenstein': 'Europe',\n",
    "    'Lithuania': 'Europe',\n",
    "    'Luxembourg': 'Europe',\n",
    "    'Madagascar': 'Africa',\n",
    "    'Malawi': 'Africa',\n",
    "    'Malaysia': 'Asia',\n",
    "    'Maldives': 'Asia',\n",
    "    'Mali': 'Africa',\n",
    "    'Malta': 'Europe',\n",
    "    'Marshall Islands': 'Oceania',\n",
    "    'Mauritania': 'Africa',\n",
    "    'Mauritius': 'Africa',\n",
    "    'Mexico': 'North America',\n",
    "    'Micronesia': 'Oceania',\n",
    "    'Moldova': 'Europe',\n",
    "    'Monaco': 'Europe',\n",
    "    'Mongolia': 'Asia',\n",
    "    'Montenegro': 'Europe',\n",
    "    'Morocco': 'Africa',\n",
    "    'Mozambique': 'Africa',\n",
    "    'Myanmar': 'Asia',\n",
    "    'Namibia': 'Africa',\n",
    "    'Nauru': 'Oceania',\n",
    "    'Nepal': 'Asia',\n",
    "    'Netherlands': 'Europe',\n",
    "    'New Zealand': 'Oceania',\n",
    "    'Nicaragua': 'North America',\n",
    "    'Niger': 'Africa',\n",
    "    'Nigeria': 'Africa',\n",
    "    'North Macedonia': 'Europe',\n",
    "    'Norway': 'Europe',\n",
    "    'Oman': 'Asia',\n",
    "    'Pakistan': 'Asia',\n",
    "    'Palau': 'Oceania',\n",
    "    'Palestine': 'Asia',\n",
    "    'Panama': 'North America',\n",
    "    'Papua New Guinea': 'Oceania',\n",
    "    'Paraguay': 'South America',\n",
    "    'Peru': 'South America',\n",
    "    'Philippines': 'Asia',\n",
    "    'Poland': 'Europe',\n",
    "    'Portugal': 'Europe',\n",
    "    'Qatar': 'Asia',\n",
    "    'Romania': 'Europe',\n",
    "    'Russia': 'Europe',\n",
    "    'Rwanda': 'Africa',\n",
    "    'Saint Kitts and Nevis': 'North America',\n",
    "    'Saint Lucia': 'North America',\n",
    "    'Saint Vincent and the Grenadines': 'North America',\n",
    "    'Samoa': 'Oceania',\n",
    "    'San Marino': 'Europe',\n",
    "    'Sao Tome and Principe': 'Africa',\n",
    "    'Saudi Arabia': 'Asia',\n",
    "    'Senegal': 'Africa',\n",
    "    'Serbia': 'Europe',\n",
    "    'Seychelles': 'Africa',\n",
    "    'Sierra Leone': 'Africa',\n",
    "    'Singapore': 'Asia',\n",
    "    'Slovakia': 'Europe',\n",
    "    'Slovenia': 'Europe',\n",
    "    'Solomon Islands': 'Oceania',\n",
    "    'Somalia': 'Africa',\n",
    "    'South Africa': 'Africa',\n",
    "    'South Sudan': 'Africa',\n",
    "    'Spain': 'Europe',\n",
    "    'Sri Lanka': 'Asia',\n",
    "    'Sudan': 'Africa',\n",
    "    'Suriname': 'South America',\n",
    "    'Sweden': 'Europe',\n",
    "    'Switzerland': 'Europe',\n",
    "    'Syria': 'Asia',\n",
    "    'Taiwan': 'Asia',\n",
    "    'Tajikistan': 'Asia',\n",
    "    'Tanzania': 'Africa',\n",
    "    'Thailand': 'Asia',\n",
    "    'Timor-Leste': 'Asia',\n",
    "    'Togo': 'Africa',\n",
    "    'Tonga': 'Oceania',\n",
    "    'Trinidad and Tobago': 'North America',\n",
    "    'Tunisia': 'Africa',\n",
    "    'Turkey': 'Asia',\n",
    "    'Turkmenistan': 'Asia',\n",
    "    'Tuvalu': 'Oceania',\n",
    "    'Uganda': 'Africa',\n",
    "    'Ukraine': 'Europe',\n",
    "    'United Arab Emirates': 'Asia',\n",
    "    'United Kingdom': 'Europe',\n",
    "    'United States': 'North America',\n",
    "    'Uruguay': 'South America',\n",
    "    'Uzbekistan': 'Asia',\n",
    "    'Vanuatu': 'Oceania',\n",
    "    'Vatican City': 'Europe',\n",
    "    'Venezuela': 'South America',\n",
    "    'Vietnam': 'Asia',\n",
    "    'Yemen': 'Asia',\n",
    "    'Zambia': 'Africa',\n",
    "    'Zimbabwe': 'Africa'\n",
    "}\n",
    "\n",
    "# Thêm cột châu lục vào DataFrame\n",
    "df_final['Continent'] = df_final['Country_of_Origin'].map(continent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1c9c3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa các khoảng khối lượng (theo ounces)\n",
    "bins = [0, 16, 320, 800, 2400, float('inf')]\n",
    "\n",
    "# Định nghĩa nhãn cho các khoảng khối lượng\n",
    "labels = ['Small standard-size', 'Large standard-size', 'Large bulky', 'Extra-large', 'Extra-large 150+ lb']\n",
    "\n",
    "# Thêm cột ProductSizeTier vào DataFrame df_final\n",
    "df_final['ProductSizeTier'] = pd.cut(df_final['Item_Weight'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Định nghĩa các category cho cột ProductSizeTier\n",
    "categories = pd.CategoricalDtype(categories=labels, ordered=True)\n",
    "\n",
    "# Chuyển đổi cột ProductSizeTier sang dạng categorical với các category đã được định nghĩa\n",
    "df_final['ProductSizeTier'] = df_final['ProductSizeTier'].astype(categories)\n",
    "\n",
    "# Thêm giá trị \"Undefined\" vào cột ProductSizeTier nếu không có giá trị cụ thể\n",
    "df_final['ProductSizeTier'] = df_final['ProductSizeTier'].cat.add_categories(['Undefined']).fillna('Undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "193393cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chuyển cột Price_x sang kiểu số, nếu có lỗi sẽ chuyển thành NaN\n",
    "df_final['Price_x'] = pd.to_numeric(df_final['Price_x'], errors='coerce')\n",
    "\n",
    "# Bạn có thể xóa các dòng có giá trị NaN trong cột 'Price_x' nếu cần\n",
    "# df_final = df_final.dropna(subset=['Price_x'])\n",
    "\n",
    "# Định nghĩa các bin và nhãn\n",
    "bins = [0, 10, 50, 150, 500, float('inf')]\n",
    "labels = ['Very Low Income', 'Low Income', 'Lower-Middle Income', 'Upper-Middle Income', 'High Income']\n",
    "\n",
    "# Thêm cột PriceGroup vào dataframe dựa trên các bin và nhãn\n",
    "df_final['PriceGroup'] = pd.cut(df_final['Price_x'], bins=bins, labels=labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8ecdf646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Hàm để chuyển đổi chuỗi ngày tháng sang định dạng ngày tháng\n",
    "def convert_to_date(date_value):\n",
    "    # Kiểm tra nếu là None, NaN hoặc không phải chuỗi\n",
    "    if pd.isna(date_value) or not isinstance(date_value, str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Chuyển đổi chuỗi thành định dạng ngày tháng\n",
    "        date_object = datetime.datetime.strptime(date_value, '%B %d, %Y').date()\n",
    "        return date_object\n",
    "    except ValueError:\n",
    "        # Thử các định dạng khác nếu định dạng đầu tiên không phù hợp\n",
    "        try:\n",
    "            date_object = datetime.datetime.strptime(date_value, '%b %d, %Y').date()\n",
    "            return date_object\n",
    "        except ValueError:\n",
    "            try:\n",
    "                date_object = datetime.datetime.strptime(date_value, '%m/%d/%Y').date()\n",
    "                return date_object\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "# Đảm bảo chỉ chuyển đổi các giá trị chuỗi\n",
    "df_final['Date_First_Available'] = df_final['Date_First_Available'].apply(convert_to_date)\n",
    "# Chuyển đổi cột thành kiểu datetime với `errors='coerce'` để thay thế các giá trị không hợp lệ bằng NaT\n",
    "df_final['Date_First_Available'] = pd.to_datetime(df_final['Date_First_Available'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c8e51e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.rename(columns={'Price_y': 'PriceDetail'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "24a984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Color'] = df_final['Color'].fillna('Tone Free')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e413998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Sub Department</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price_x</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Day Delivered</th>\n",
       "      <th>Date Added</th>\n",
       "      <th>...</th>\n",
       "      <th>5-star</th>\n",
       "      <th>4-star</th>\n",
       "      <th>3-star</th>\n",
       "      <th>2-star</th>\n",
       "      <th>1-star</th>\n",
       "      <th>Main Ranking Value</th>\n",
       "      <th>volume_cm3</th>\n",
       "      <th>Continent</th>\n",
       "      <th>ProductSizeTier</th>\n",
       "      <th>PriceGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Car Care</td>\n",
       "      <td>1200 GSM Microfiber Car Drying Towel – Extra L...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/WEST-HORSE-Microfiber-D...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81YuzjzMln...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1059370.0</td>\n",
       "      <td>1415.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Car Care</td>\n",
       "      <td>2-Pack Microfiber Car Drying Towel – Extra Lar...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/WEST-HORSE-2-Pack-Micro...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81AyrN0G9M...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205921.0</td>\n",
       "      <td>1415.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Car Care</td>\n",
       "      <td>2Pcs Car Scratch Remover Repair Paste Car Buff...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/Scratch-Remover-Repair-...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71fuDYoGV+...</td>\n",
       "      <td>24</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>917.90</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Small standard-size</td>\n",
       "      <td>Very Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Car Care</td>\n",
       "      <td>Nano Sparkle Cloth Car Scratch Remover, Car Bu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/Sparkle-Scratch-Remover...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/616AjTuktS...</td>\n",
       "      <td>24</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>214376.0</td>\n",
       "      <td>562.16</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Small standard-size</td>\n",
       "      <td>Very Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Car Care</td>\n",
       "      <td>Black Dry Guide Coat Kit Auto Body Guide Coat ...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.amazon.com/Xanadued-Black-Guide-Co...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61YQlI1DbJ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2461.53</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87433</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Tools &amp; Equipment</td>\n",
       "      <td>TOOLIOM 3-Tier Tilt-Table Welding Cart for Tig...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>https://www.amazon.com/TOOLIOM-Welding-Drawers...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71fvu2xZFB...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>279391.0</td>\n",
       "      <td>27800.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Lower-Middle Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87435</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Tools &amp; Equipment</td>\n",
       "      <td>Master Hex Bit Socket Set, S2 Alloy Steel, Com...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>https://www.amazon.com/KOOPOOL-Complete-32-Pie...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81AabAtDon...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300706.0</td>\n",
       "      <td>3841.39</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Low Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87437</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Tools &amp; Equipment</td>\n",
       "      <td>LAUNCH X431 CRP919E Lite Bidirectional Scan To...</td>\n",
       "      <td>385.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.amazon.com/LAUNCH-CRP919E-Lite-Bid...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/812Vc1wRMZ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>394382.0</td>\n",
       "      <td>18018.95</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Upper-Middle Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87439</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Tools &amp; Equipment</td>\n",
       "      <td>DEWALT Portable Electric Vehicle (EV) 120-240V...</td>\n",
       "      <td>399.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>69.0</td>\n",
       "      <td>https://www.amazon.com/DEWALT-Portable-Electri...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61WLW6S3Tl...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>80213.0</td>\n",
       "      <td>20772.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Upper-Middle Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87441</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>Tools &amp; Equipment</td>\n",
       "      <td>Macnaught Heavy Duty Pistol Grip Grease Gun wi...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>207.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61fcuA2xkV...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>765.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small standard-size</td>\n",
       "      <td>Lower-Middle Income</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65133 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Department     Sub Department  \\\n",
       "0      Automotive           Car Care   \n",
       "1      Automotive           Car Care   \n",
       "2      Automotive           Car Care   \n",
       "3      Automotive           Car Care   \n",
       "4      Automotive           Car Care   \n",
       "...           ...                ...   \n",
       "87433  Automotive  Tools & Equipment   \n",
       "87435  Automotive  Tools & Equipment   \n",
       "87437  Automotive  Tools & Equipment   \n",
       "87439  Automotive  Tools & Equipment   \n",
       "87441  Automotive  Tools & Equipment   \n",
       "\n",
       "                                            Product Name  Price_x  Rating  \\\n",
       "0      1200 GSM Microfiber Car Drying Towel – Extra L...     29.0     NaN   \n",
       "1      2-Pack Microfiber Car Drying Towel – Extra Lar...     23.0     NaN   \n",
       "2      2Pcs Car Scratch Remover Repair Paste Car Buff...      9.0     NaN   \n",
       "3      Nano Sparkle Cloth Car Scratch Remover, Car Bu...      9.0     NaN   \n",
       "4      Black Dry Guide Coat Kit Auto Body Guide Coat ...     31.0     NaN   \n",
       "...                                                  ...      ...     ...   \n",
       "87433  TOOLIOM 3-Tier Tilt-Table Welding Cart for Tig...     80.0     3.8   \n",
       "87435  Master Hex Bit Socket Set, S2 Alloy Steel, Com...     27.0     4.9   \n",
       "87437  LAUNCH X431 CRP919E Lite Bidirectional Scan To...    385.0     4.8   \n",
       "87439  DEWALT Portable Electric Vehicle (EV) 120-240V...    399.0     4.3   \n",
       "87441  Macnaught Heavy Duty Pistol Grip Grease Gun wi...     67.0     4.4   \n",
       "\n",
       "       Review                                               Link  \\\n",
       "0         NaN  https://www.amazon.com/WEST-HORSE-Microfiber-D...   \n",
       "1         NaN  https://www.amazon.com/WEST-HORSE-2-Pack-Micro...   \n",
       "2         NaN  https://www.amazon.com/Scratch-Remover-Repair-...   \n",
       "3         NaN  https://www.amazon.com/Sparkle-Scratch-Remover...   \n",
       "4         NaN  https://www.amazon.com/Xanadued-Black-Guide-Co...   \n",
       "...       ...                                                ...   \n",
       "87433    12.0  https://www.amazon.com/TOOLIOM-Welding-Drawers...   \n",
       "87435    15.0  https://www.amazon.com/KOOPOOL-Complete-32-Pie...   \n",
       "87437    13.0  https://www.amazon.com/LAUNCH-CRP919E-Lite-Bid...   \n",
       "87439    69.0  https://www.amazon.com/DEWALT-Portable-Electri...   \n",
       "87441   207.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "\n",
       "                                               Image URL  Day Delivered  \\\n",
       "0      https://m.media-amazon.com/images/I/81YuzjzMln...              0   \n",
       "1      https://m.media-amazon.com/images/I/81AyrN0G9M...              0   \n",
       "2      https://m.media-amazon.com/images/I/71fuDYoGV+...             24   \n",
       "3      https://m.media-amazon.com/images/I/616AjTuktS...             24   \n",
       "4      https://m.media-amazon.com/images/I/61YQlI1DbJ...             68   \n",
       "...                                                  ...            ...   \n",
       "87433  https://m.media-amazon.com/images/I/71fvu2xZFB...              0   \n",
       "87435  https://m.media-amazon.com/images/I/81AabAtDon...              0   \n",
       "87437  https://m.media-amazon.com/images/I/812Vc1wRMZ...              0   \n",
       "87439  https://m.media-amazon.com/images/I/61WLW6S3Tl...              0   \n",
       "87441  https://m.media-amazon.com/images/I/61fcuA2xkV...              0   \n",
       "\n",
       "       Date Added  ... 5-star 4-star 3-star 2-star 1-star Main Ranking Value  \\\n",
       "0      2025-03-22  ...    100      0      0      0      0          1059370.0   \n",
       "1      2025-03-22  ...     82     10      8      0      0           205921.0   \n",
       "2      2025-03-22  ...      0      0      0      0      0                NaN   \n",
       "3      2025-03-22  ...      0      0      0      0      0           214376.0   \n",
       "4      2025-03-22  ...      0      0      0      0      0                NaN   \n",
       "...           ...  ...    ...    ...    ...    ...    ...                ...   \n",
       "87433  2025-03-22  ...     49     21     10     10     10           279391.0   \n",
       "87435  2025-03-22  ...     80     20      0      0      0           300706.0   \n",
       "87437  2025-03-22  ...     87     13      0      0      0           394382.0   \n",
       "87439  2025-03-22  ...     71     14      6      0      9            80213.0   \n",
       "87441  2025-03-22  ...    100      0      0      0      0                NaN   \n",
       "\n",
       "      volume_cm3 Continent      ProductSizeTier           PriceGroup  \n",
       "0        1415.84       NaN            Undefined           Low Income  \n",
       "1        1415.84       NaN            Undefined           Low Income  \n",
       "2         917.90      Asia  Small standard-size      Very Low Income  \n",
       "3         562.16      Asia  Small standard-size      Very Low Income  \n",
       "4        2461.53      Asia            Undefined           Low Income  \n",
       "...          ...       ...                  ...                  ...  \n",
       "87433   27800.65       NaN            Undefined  Lower-Middle Income  \n",
       "87435    3841.39      Asia            Undefined           Low Income  \n",
       "87437   18018.95      Asia            Undefined  Upper-Middle Income  \n",
       "87439   20772.74       NaN            Undefined  Upper-Middle Income  \n",
       "87441     765.73       NaN  Small standard-size  Lower-Middle Income  \n",
       "\n",
       "[65133 rows x 32 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.drop(columns={'PriceDetail','Item_Model_Number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "10ef39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"stg_amz.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all dimension + fact tables into CSV at: ./csv_output\n"
     ]
    }
   ],
   "source": [
    "def save_olap_csv(df_final, output_dir='./csv_output'):\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_final = df_final.dropna(subset= 'ASIN')\n",
    "    # dim_product\n",
    "    dim_product = df_final[['ASIN', 'Product Name', 'Image URL', 'Color','Description','Rating', 'Review', '5-star', '4-star', '3-star', '2-star', '1-star', 'Item_Weight', 'volume_cm3','ProductSizeTier']].drop_duplicates().rename(columns={\n",
    "        'Product Name': 'Product_Name',\n",
    "        'Image URL': 'Image_URL'\n",
    "        'Rating': 'Overall_Rating',\n",
    "        'Review': 'Review_Count',\n",
    "        '5-star': '5Star',\n",
    "        '4-star': '4Star',\n",
    "        '3-star': '3Star',\n",
    "        '2-star': '2Star',\n",
    "        '1-star': '1Star',\n",
    "        'Item_Weight': 'Weight',\n",
    "        'volume_cm3': 'Volumn'\n",
    "    })\n",
    "    dim_product.to_csv(f'{output_dir}/dim_product.csv', index=False)\n",
    "\n",
    "    # dim_category\n",
    "    dim_category = df_final[['Sub Department']].drop_duplicates().rename(columns={'Sub Department': 'Category'})\n",
    "    dim_category['CategoryID'] = range(1, len(dim_category) + 1)\n",
    "    dim_category.to_csv(f'{output_dir}/dim_category.csv', index=False)\n",
    "\n",
    "    # dim_date\n",
    "    dim_date = df_final[['Date_First_Available']].drop_duplicates().rename(columns={'Date_First_Available': 'Date'})\n",
    "    # Loại bỏ các dòng có giá trị NaT (NaN trong datetime)\n",
    "    #dim_date = dim_date.dropna(subset=['Date'])\n",
    "    # Thêm cột year, quarter, month, day\n",
    "    dim_date['year'] = dim_date['Date'].dt.year  # Sửa lỗi tại đây\n",
    "    dim_date['quarter'] = dim_date['Date'].dt.quarter\n",
    "    dim_date['month'] = dim_date['Date'].dt.month\n",
    "    dim_date['day'] = dim_date['Date'].dt.day\n",
    "    dim_date['dateId'] = range(1, len(dim_date) + 1)\n",
    "    dim_date.to_csv(f'{output_dir}/dim_date.csv', index=False)\n",
    "\n",
    "    # dim_geolocation\n",
    "    dim_geolocation = df_final[['Country_of_Origin', 'Continent']].drop_duplicates().rename(columns={\n",
    "        'Country_of_Origin': 'CountryOfOrigin'\n",
    "    })\n",
    "    #dim_geolocation = dim_geolocation.dropna(subset=['CountryOfOrigin'])\n",
    "    dim_geolocation['locationID'] = range(1, len(dim_geolocation) + 1)\n",
    "    dim_geolocation.to_csv(f'{output_dir}/dim_geolocation.csv', index=False)\n",
    "\n",
    "    # dim_manufacturer\n",
    "    dim_manufacturer = df_final[['Manufacturer']].drop_duplicates()\n",
    "   # dim_manufacturer = dim_manufacturer.dropna(subset=['Manufacturer'])\n",
    "    dim_manufacturer['manufacturerID'] = range(1, len(dim_manufacturer) + 1)\n",
    "    dim_manufacturer.to_csv(f'{output_dir}/dim_manufacturer.csv', index=False)\n",
    "\n",
    "    # dim_brand\n",
    "    dim_brand = df_final[['Store']].drop_duplicates().rename(columns={'Store': 'Brand'})\n",
    "  #  dim_brand = dim_brand.dropna(subset='Brand')\n",
    "    dim_brand['brandID'] = range(1, len(dim_brand) + 1)\n",
    "    dim_brand.to_csv(f'{output_dir}/dim_brand.csv', index=False)\n",
    "\n",
    "    # dim_product_category\n",
    "    category_map = dict(zip(dim_category['Category'], dim_category['CategoryID']))\n",
    "    dim_product_category = df_final[['ASIN', 'Sub Department']].drop_duplicates()\n",
    "    dim_product_category['CategoryID'] = dim_product_category['Sub Department'].map(category_map)\n",
    "    dim_product_category = dim_product_category[['ASIN', 'CategoryID']].dropna().astype({'CategoryID': 'int'})\n",
    "    dim_product_category.to_csv(f'{output_dir}/dim_product_category.csv', index=False)\n",
    "\n",
    "    # mapping\n",
    "    date_map = dict(zip(dim_date['Date'], dim_date['dateId']))\n",
    "    location_map = dict(zip(df_final['Country_of_Origin'], dim_geolocation['locationID']))\n",
    "    manufacturer_map = dict(zip(dim_manufacturer['Manufacturer'], dim_manufacturer['manufacturerID']))\n",
    "    brand_map = dict(zip(dim_brand['Brand'], dim_brand['brandID']))\n",
    "\n",
    "    # fact_product_overview\n",
    "    fact_product_overview = df_final[['ASIN', 'Sub Department', 'Country_of_Origin', 'Continent', 'Store', 'Manufacturer', 'Date_First_Available']].copy()\n",
    "    fact_product_overview['CategoryID'] = fact_product_overview['Sub Department'].map(category_map)\n",
    "    fact_product_overview['dateId'] = fact_product_overview['Date_First_Available'].map(date_map)\n",
    "    fact_product_overview['locationID'] = (fact_product_overview['Country_of_Origin']).map(location_map)\n",
    "    fact_product_overview['brandID'] = fact_product_overview['Store'].map(brand_map)\n",
    "    fact_product_overview['manufacturerID'] = fact_product_overview['Manufacturer'].map(manufacturer_map)\n",
    "    fact_product_overview = fact_product_overview[['ASIN', 'locationID', 'CategoryID', 'brandID', 'manufacturerID', 'dateId']]\n",
    "    fact_product_overview.to_csv(f'{output_dir}/fact_product_overview.csv', index=False)\n",
    "\n",
    "\n",
    "    # fact_sale\n",
    "    fact_sale = df_final[['ASIN', 'Price_x', 'Sub Department', 'Country_of_Origin', 'Continent', 'Store', 'Manufacturer', 'Date_First_Available']].copy()\n",
    "    fact_sale['discount_percentage'] = 0.0\n",
    "    fact_sale['price_before_discount'] = fact_sale['Price_x']\n",
    "    fact_sale['price'] = fact_sale['Price_x']\n",
    "    fact_sale['CategoryID'] = fact_sale['Sub Department'].map(category_map)\n",
    "    fact_sale['dateId'] = fact_sale['Date_First_Available'].map(date_map)\n",
    "    fact_sale['locationID'] = (fact_sale['Country_of_Origin'] + '_' + fact_sale['Continent']).map(location_map)\n",
    "    fact_sale['brandID'] = fact_sale['Store'].map(brand_map)\n",
    "    fact_sale['manufacturerID'] = fact_sale['Manufacturer'].map(manufacturer_map)\n",
    "    fact_sale = fact_sale[['ASIN', 'price_before_discount', 'discount_percentage', 'price', 'locationID', 'CategoryID', 'brandID', 'dateId', 'manufacturerID']]\n",
    "    fact_sale.to_csv(f'{output_dir}/fact_sale.csv', index=False)\n",
    "\n",
    "    # fact_customer_experience\n",
    "    fact_customer_experience = df_final[['ASIN', 'Date_First_Available','Sub Department', 'Main Ranking Value', 'Price_x','Amazon_Global_Shipping', 'Estimated_Import_Charges','PriceGroup','Day Delivered']].copy()\n",
    "    fact_customer_experience['dateId'] = fact_customer_experience['Date_First_Available'].map(date_map)\n",
    "    fact_customer_experience['price'] = fact_customer_experience['Price_x']\n",
    "    fact_customer_experience['CategoryID'] = fact_customer_experience['Sub Department'].map(category_map)\n",
    "    fact_customer_experience['Delivery_time_days'] = fact_customer_experience['Day Delivered']\n",
    "    fact_customer_experience = fact_customer_experience.rename(columns={\n",
    "        'Main Ranking Value': 'MainRankingValue',\n",
    "        'Amazon_Global_Shipping': 'AmazonGlobal_Shipping',\n",
    "        'Estimated_Import_Charges': 'Estimated_Import_Charge'\n",
    "    })\n",
    "    fact_customer_experience = fact_customer_experience[['ASIN', 'dateId', 'CategoryID','Delivery_time_days' ,'MainRankingValue','AmazonGlobal_Shipping', 'Estimated_Import_Charge','PriceGroup']]\n",
    "    fact_customer_experience.to_csv(f'{output_dir}/fact_customer_experience.csv', index=False)\n",
    "\n",
    "    print(f\"✅ Saved all dimension + fact tables into CSV at: {output_dir}\")\n",
    "save_olap_csv(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9998e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL is successful.\n",
      "Truncating all tables before loading data...\n",
      "✅ Foreign key checks disabled.\n",
      "Truncating table fact_customer_experience...\n",
      "✅ Table fact_customer_experience truncated successfully.\n",
      "Truncating table fact_sale...\n",
      "✅ Table fact_sale truncated successfully.\n",
      "Truncating table fact_product_overview...\n",
      "✅ Table fact_product_overview truncated successfully.\n",
      "Truncating table dim_product_category...\n",
      "✅ Table dim_product_category truncated successfully.\n",
      "Truncating table dim_product...\n",
      "✅ Table dim_product truncated successfully.\n",
      "Truncating table dim_measurements...\n",
      "✅ Table dim_measurements truncated successfully.\n",
      "Truncating table dim_ratings...\n",
      "✅ Table dim_ratings truncated successfully.\n",
      "Truncating table dim_geolocation...\n",
      "✅ Table dim_geolocation truncated successfully.\n",
      "Truncating table dim_brand...\n",
      "✅ Table dim_brand truncated successfully.\n",
      "Truncating table dim_manufacturer...\n",
      "✅ Table dim_manufacturer truncated successfully.\n",
      "Truncating table dim_category...\n",
      "✅ Table dim_category truncated successfully.\n",
      "Truncating table dim_date...\n",
      "✅ Table dim_date truncated successfully.\n",
      "✅ Foreign key checks re-enabled.\n",
      "Loading ./csv_output/dim_date.csv into dim_date...\n",
      "Read 2214 rows from ./csv_output/dim_date.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_date (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_date (Total: 2000)\n",
      "✅ Inserted chunk of 214 rows into dim_date (Total: 2214)\n",
      "✅ Completed inserting all 2214 rows into dim_date.\n",
      "Loading ./csv_output/dim_category.csv into dim_category...\n",
      "Read 13 rows from ./csv_output/dim_category.csv\n",
      "✅ Inserted chunk of 13 rows into dim_category (Total: 13)\n",
      "✅ Completed inserting all 13 rows into dim_category.\n",
      "Loading ./csv_output/dim_manufacturer.csv into dim_manufacturer...\n",
      "Read 44317 rows from ./csv_output/dim_manufacturer.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into dim_manufacturer (Total: 44000)\n",
      "✅ Inserted chunk of 317 rows into dim_manufacturer (Total: 44317)\n",
      "✅ Completed inserting all 44317 rows into dim_manufacturer.\n",
      "Loading ./csv_output/dim_brand.csv into dim_brand...\n",
      "Read 14054 rows from ./csv_output/dim_brand.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_brand (Total: 14000)\n",
      "✅ Inserted chunk of 54 rows into dim_brand (Total: 14054)\n",
      "✅ Completed inserting all 14054 rows into dim_brand.\n",
      "Loading ./csv_output/dim_geolocation.csv into dim_geolocation...\n",
      "Read 55 rows from ./csv_output/dim_geolocation.csv\n",
      "✅ Inserted chunk of 55 rows into dim_geolocation (Total: 55)\n",
      "✅ Completed inserting all 55 rows into dim_geolocation.\n",
      "Loading ./csv_output/dim_ratings.csv into dim_ratings...\n",
      "Read 65132 rows from ./csv_output/dim_ratings.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into dim_ratings (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into dim_ratings (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into dim_ratings.\n",
      "Loading ./csv_output/dim_measurements.csv into dim_measurements...\n",
      "Read 65132 rows from ./csv_output/dim_measurements.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into dim_measurements (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into dim_measurements (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into dim_measurements.\n",
      "Loading ./csv_output/dim_product.csv into dim_product...\n",
      "Read 65132 rows from ./csv_output/dim_product.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into dim_product (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into dim_product.\n",
      "Loading ./csv_output/dim_product_category.csv into dim_product_category...\n",
      "Read 65132 rows from ./csv_output/dim_product_category.csv\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into dim_product_category (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into dim_product_category (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into dim_product_category.\n",
      "Loading ./csv_output/fact_product_overview.csv into fact_product_overview...\n",
      "Read 65132 rows from ./csv_output/fact_product_overview.csv\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into fact_product_overview (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into fact_product_overview (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into fact_product_overview.\n",
      "Loading ./csv_output/fact_sale.csv into fact_sale...\n",
      "Read 65132 rows from ./csv_output/fact_sale.csv\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into fact_sale (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into fact_sale (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into fact_sale.\n",
      "Loading ./csv_output/fact_customer_experience.csv into fact_customer_experience...\n",
      "Read 65132 rows from ./csv_output/fact_customer_experience.csv\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 1000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 2000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 3000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 4000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 5000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 6000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 7000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 8000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 9000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 10000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 11000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 12000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 13000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 14000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 15000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 16000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 17000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 18000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 19000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 20000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 21000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 22000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 23000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 24000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 25000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 26000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 27000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 28000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 29000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 30000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 31000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 32000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 33000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 34000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 35000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 36000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 37000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 38000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 39000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 40000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 41000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 42000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 43000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 44000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 45000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 46000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 47000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 48000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 49000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 50000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 51000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 52000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 53000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 54000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 55000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 56000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 57000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 58000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 59000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 60000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 61000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 62000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 63000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 64000)\n",
      "✅ Inserted chunk of 1000 rows into fact_customer_experience (Total: 65000)\n",
      "✅ Inserted chunk of 132 rows into fact_customer_experience (Total: 65132)\n",
      "✅ Completed inserting all 65132 rows into fact_customer_experience.\n",
      "Connection to MySQL closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Define the connection to MySQL with extended timeout\n",
    "def create_connection():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='olap_datn',\n",
    "            user='root',\n",
    "            password='phuocle11001203',\n",
    "            connect_timeout=360,  # Tăng timeout lên 180 giây\n",
    "            allow_local_infile=True\n",
    "        )\n",
    "        if connection.is_connected():\n",
    "            # Thiết lập thời gian chờ lâu hơn cho phiên làm việc\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(\"SET SESSION wait_timeout=600\")  # 10 phút\n",
    "            cursor.execute(\"SET SESSION interactive_timeout=600\")  # 10 phút\n",
    "            cursor.close()\n",
    "            print(\"Connection to MySQL is successful.\")\n",
    "            return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to disable foreign key checks\n",
    "def disable_foreign_key_checks(connection):\n",
    "    if connection is None or not connection.is_connected():\n",
    "        return False\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SET FOREIGN_KEY_CHECKS=0\")\n",
    "        connection.commit()\n",
    "        print(\"✅ Foreign key checks disabled.\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"❌ Error disabling foreign key checks: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Function to enable foreign key checks\n",
    "def enable_foreign_key_checks(connection):\n",
    "    if connection is None or not connection.is_connected():\n",
    "        return False\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SET FOREIGN_KEY_CHECKS=1\")\n",
    "        connection.commit()\n",
    "        print(\"✅ Foreign key checks re-enabled.\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"❌ Error enabling foreign key checks: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Function to truncate a table\n",
    "def truncate_table(table_name, connection):\n",
    "    if connection is None or not connection.is_connected():\n",
    "        print(f\"Cannot truncate {table_name}: No active connection\")\n",
    "        return False\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        print(f\"Truncating table {table_name}...\")\n",
    "        cursor.execute(f\"TRUNCATE TABLE {table_name}\")\n",
    "        connection.commit()\n",
    "        print(f\"✅ Table {table_name} truncated successfully.\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"❌ Error truncating table {table_name}: {e}\")\n",
    "        connection.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Truncate tables in the correct order, with FK checks disabled\n",
    "def truncate_all_tables(connection):\n",
    "    if not disable_foreign_key_checks(connection):\n",
    "        print(\"Cannot proceed with truncating tables without disabling FK checks.\")\n",
    "        return False\n",
    "    \n",
    "    # Đảo ngược thứ tự bảng để truncate fact tables trước\n",
    "    tables_to_truncate = [\n",
    "        'fact_customer_experience',\n",
    "        'fact_sale',\n",
    "        'fact_product_overview',\n",
    "        'dim_product_category',\n",
    "        'dim_product',\n",
    "        'dim_measurements',\n",
    "        'dim_geolocation',\n",
    "        'dim_brand',\n",
    "        'dim_manufacturer',\n",
    "        'dim_category',\n",
    "        'dim_date'\n",
    "    ]\n",
    "    \n",
    "    all_succeeded = True\n",
    "    for table_name in tables_to_truncate:\n",
    "        truncate_table(table_name, connection)\n",
    "    \n",
    "    # Re-enable foreign key checks\n",
    "    enable_foreign_key_checks(connection)\n",
    "    \n",
    "    return all_succeeded\n",
    "\n",
    "# Fix NaN values in DataFrame\n",
    "def fix_nan_values(df):\n",
    "    # Thay thế NaN bằng None (SQL NULL) trong toàn bộ DataFrame\n",
    "    return df.replace({np.nan: None})\n",
    "\n",
    "# Function to load CSV files into MySQL tables with chunking\n",
    "def load_csv_to_mysql(df, table_name, connection, truncate_first=False):\n",
    "    if connection is None or not connection.is_connected():\n",
    "        print(f\"Cannot load data into {table_name}: No active connection\")\n",
    "        return False\n",
    "    \n",
    "    # Fix NaN values before loading\n",
    "    df = fix_nan_values(df)\n",
    "    \n",
    "    # Truncate table if requested (we'll handle this separately now)\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    chunk_size = 1000  # Điều chỉnh số lượng bản ghi trong mỗi lần chèn\n",
    "    rows_inserted = 0\n",
    "    \n",
    "    try:\n",
    "        # Xử lý theo từng khối nhỏ\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            \n",
    "            # Chuẩn bị truy vấn chèn\n",
    "            columns = ', '.join([f\"`{col}`\" for col in chunk.columns])  # Bọc tên cột bằng backticks\n",
    "            placeholders = ', '.join(['%s'] * len(chunk.columns))\n",
    "            insert_query = f\"INSERT IGNORE INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            \n",
    "            # Chuyển đổi DataFrame thành danh sách các hàng\n",
    "            values = [tuple(row) for row in chunk.values]\n",
    "            \n",
    "            # Thực thi truy vấn\n",
    "            cursor.executemany(insert_query, values)\n",
    "            connection.commit()\n",
    "            \n",
    "            rows_inserted += len(chunk)\n",
    "            print(f\"✅ Inserted chunk of {len(chunk)} rows into {table_name} (Total: {rows_inserted})\")\n",
    "            \n",
    "            # Tạm dừng giữa các khối để tránh quá tải\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        print(f\"✅ Completed inserting all {rows_inserted} rows into {table_name}.\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"❌ Error inserting into {table_name}: {e}\")\n",
    "        connection.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Load CSVs and insert into MySQL\n",
    "def load_data_into_mysql(output_dir='./csv_output', truncate_tables=True):\n",
    "    # Danh sách các bảng theo thứ tự phù hợp (bảng fact sau cùng vì chúng phụ thuộc vào bảng dimension)\n",
    "    tables_order = [\n",
    "        'dim_date',\n",
    "        'dim_category',\n",
    "        'dim_manufacturer',\n",
    "        'dim_brand',\n",
    "        'dim_geolocation',\n",
    "        'dim_measurements',\n",
    "        'dim_product',\n",
    "        'dim_product_category',\n",
    "        'fact_product_overview',\n",
    "        'fact_sale',\n",
    "        'fact_customer_experience'\n",
    "    ]\n",
    "    \n",
    "    tables = {table: f\"{table}.csv\" for table in tables_order}\n",
    "    \n",
    "    # Tạo kết nối MySQL\n",
    "    connection = create_connection()\n",
    "    if connection is None:\n",
    "        return\n",
    "    \n",
    "    # Truncate tất cả các bảng trước nếu được yêu cầu\n",
    "    if truncate_tables:\n",
    "        print(\"Truncating all tables before loading data...\")\n",
    "        truncate_all_tables(connection)\n",
    "    \n",
    "    # Xử lý từng bảng theo thứ tự đã định\n",
    "    for table_name in tables_order:\n",
    "            \n",
    "        file_name = tables[table_name]\n",
    "        file_path = f\"{output_dir}/{file_name}\"\n",
    "        try:\n",
    "            print(f\"Loading {file_path} into {table_name}...\")\n",
    "            \n",
    "            # Đọc file CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Read {len(df)} rows from {file_path}\")\n",
    "            \n",
    "            # Đảm bảo kết nối vẫn hoạt động\n",
    "            if not connection.is_connected():\n",
    "                print(\"Reconnecting to MySQL...\")\n",
    "                connection = create_connection()\n",
    "                if connection is None:\n",
    "                    print(\"Failed to reconnect. Stopping.\")\n",
    "                    break\n",
    "            \n",
    "            # Load dữ liệu vào bảng\n",
    "            success = load_csv_to_mysql(df, table_name, connection, truncate_first=False)\n",
    "            if not success:\n",
    "                print(f\"Failed to load data into {table_name}. Trying to reconnect...\")\n",
    "                connection.close()\n",
    "                connection = create_connection()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data from {file_name}: {e}\")\n",
    "    \n",
    "    # Đóng kết nối\n",
    "    if connection and connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"Connection to MySQL closed.\")\n",
    "\n",
    "# Gọi hàm để tải dữ liệu vào MySQL\n",
    "if __name__ == \"__main__\":\n",
    "    # Đặt truncate_tables=True nếu muốn truncate các bảng trước khi tải dữ liệu\n",
    "    load_data_into_mysql(truncate_tables=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
